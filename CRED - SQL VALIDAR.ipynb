{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70aea681-2477-45f5-9ffe-1cb8f6cb1a68",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'.env_file' loaded!\n",
      "ENV 'PROD' configured!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/tsevero/notebooks/SAT_BIG_DATA/data-pipeline/batch/poc\")\n",
    "sys.path.append(\"/home/tsevero/notebooks/SAT_BIG_DATA/data-pipeline/batch/plugins\")\n",
    "sys.path.append(\"/home/tsevero/notebooks/SAT_BIG_DATA/data-pipeline/batch/dags\")\n",
    "\n",
    "#Import libs python\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from datetime import date\n",
    "\n",
    "#Import libs internas\n",
    "from utils import spark_utils_session as utils\n",
    "\n",
    "from hooks.hdfs.hdfs_helper import HdfsHelper\n",
    "from jobs.job_base_config import BaseETLJobClass\n",
    "\n",
    "import poc_helper\n",
    "poc_helper.load_env(\"PROD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4faf8a1f-48e4-4c57-a162-c839c8e3a940",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m2025-11-27T17:56:42.809697Z\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mUsing json file settings.     \u001b[0m [\u001b[0m\u001b[1m\u001b[34mroot\u001b[0m]\u001b[0m \u001b[36mloc\u001b[0m=\u001b[35mspark_utils_session.py:301\u001b[0m\n",
      "\u001b[2m2025-11-27T17:56:42.810883Z\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mExporting default ENV.        \u001b[0m [\u001b[0m\u001b[1m\u001b[34mroot\u001b[0m]\u001b[0m \u001b[36mloc\u001b[0m=\u001b[35mspark_utils_session.py:305\u001b[0m\n",
      "\u001b[2m2025-11-27T17:56:42.811410Z\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mExporting custom ENVs.        \u001b[0m [\u001b[0m\u001b[1m\u001b[34mroot\u001b[0m]\u001b[0m \u001b[36mloc\u001b[0m=\u001b[35mspark_utils_session.py:338\u001b[0m\n",
      "\u001b[2m2025-11-27T17:56:42.811955Z\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mBuilding profile 'efd_t2'.    \u001b[0m [\u001b[0m\u001b[1m\u001b[34mroot\u001b[0m]\u001b[0m \u001b[36mloc\u001b[0m=\u001b[35mspark_utils_session.py:221\u001b[0m\n",
      "\u001b[2m2025-11-27T17:56:42.812301Z\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNot enough info for building the kerberos client. Ignoring it\u001b[0m [\u001b[0m\u001b[1m\u001b[34mroot\u001b[0m]\u001b[0m \u001b[36mloc\u001b[0m=\u001b[35mspark_utils_session.py:284\u001b[0m\n",
      "Warning: Ignoring non-Spark config property: hive.metastore.uris\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/cloudera/parcels/SPARK3-3.5.4.3.5.7191000.0-30-1.p0.68499982/lib/spark3/jars/ivy-2.5.2.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/tsevero/.ivy2/cache\n",
      "The jars for the packages stored in: /home/tsevero/.ivy2/jars\n",
      "com.databricks#spark-xml_2.12 added as a dependency\n",
      "org.apache.iceberg#iceberg-spark-runtime-3.5_2.12 added as a dependency\n",
      "com.oracle.database.security#oraclepki added as a dependency\n",
      "com.oracle.database.security#osdt_core added as a dependency\n",
      "com.oracle.database.security#osdt_cert added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-864078dc-2c00-4353-bd95-63346bcb5cfd;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.databricks#spark-xml_2.12;0.18.0 in central\n",
      "\tfound commons-io#commons-io;2.11.0 in central\n",
      "\tfound org.glassfish.jaxb#txw2;3.0.2 in central\n",
      "\tfound org.apache.ws.xmlschema#xmlschema-core;2.3.0 in central\n",
      "\tfound org.scala-lang.modules#scala-collection-compat_2.12;2.9.0 in central\n",
      "\tfound org.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.9.2 in central\n",
      "\tfound com.oracle.database.security#oraclepki;21.19.0.0 in central\n",
      "\tfound com.oracle.database.security#osdt_core;21.19.0.0 in central\n",
      "\tfound com.oracle.database.security#osdt_cert;21.19.0.0 in central\n",
      ":: resolution report :: resolve 172ms :: artifacts dl 6ms\n",
      "\t:: modules in use:\n",
      "\tcom.databricks#spark-xml_2.12;0.18.0 from central in [default]\n",
      "\tcom.oracle.database.security#oraclepki;21.19.0.0 from central in [default]\n",
      "\tcom.oracle.database.security#osdt_cert;21.19.0.0 from central in [default]\n",
      "\tcom.oracle.database.security#osdt_core;21.19.0.0 from central in [default]\n",
      "\tcommons-io#commons-io;2.11.0 from central in [default]\n",
      "\torg.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.9.2 from central in [default]\n",
      "\torg.apache.ws.xmlschema#xmlschema-core;2.3.0 from central in [default]\n",
      "\torg.glassfish.jaxb#txw2;3.0.2 from central in [default]\n",
      "\torg.scala-lang.modules#scala-collection-compat_2.12;2.9.0 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   9   |   0   |   0   |   0   ||   9   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-864078dc-2c00-4353-bd95-63346bcb5cfd\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 9 already retrieved (0kB/5ms)\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/27 14:56:47 WARN  conf.HiveConf: [Thread-9]: HiveConf of name hive.metastore.runworker.in does not exist\n",
      "25/11/27 14:56:47 WARN  conf.HiveConf: [Thread-9]: HiveConf of name hive.masking.algo does not exist\n",
      "25/11/27 14:56:47 WARN  util.Utils: [Thread-9]: spark.executor.instances less than spark.dynamicAllocation.minExecutors is invalid, ignoring its setting, please update your configs.\n",
      "25/11/27 14:56:48 WARN  yarn.Client: [Thread-9]: Same path resource file:///home/tsevero/.ivy2/jars/com.databricks_spark-xml_2.12-0.18.0.jar added multiple times to distributed cache.\n",
      "25/11/27 14:56:48 WARN  yarn.Client: [Thread-9]: Same path resource file:///home/tsevero/.ivy2/jars/org.apache.iceberg_iceberg-spark-runtime-3.5_2.12-1.9.2.jar added multiple times to distributed cache.\n",
      "25/11/27 14:56:48 WARN  yarn.Client: [Thread-9]: Same path resource file:///home/tsevero/.ivy2/jars/com.oracle.database.security_oraclepki-21.19.0.0.jar added multiple times to distributed cache.\n",
      "25/11/27 14:56:48 WARN  yarn.Client: [Thread-9]: Same path resource file:///home/tsevero/.ivy2/jars/com.oracle.database.security_osdt_core-21.19.0.0.jar added multiple times to distributed cache.\n",
      "25/11/27 14:56:48 WARN  yarn.Client: [Thread-9]: Same path resource file:///home/tsevero/.ivy2/jars/com.oracle.database.security_osdt_cert-21.19.0.0.jar added multiple times to distributed cache.\n",
      "25/11/27 14:56:48 WARN  yarn.Client: [Thread-9]: Same path resource file:///home/tsevero/.ivy2/jars/commons-io_commons-io-2.11.0.jar added multiple times to distributed cache.\n",
      "25/11/27 14:56:48 WARN  yarn.Client: [Thread-9]: Same path resource file:///home/tsevero/.ivy2/jars/org.glassfish.jaxb_txw2-3.0.2.jar added multiple times to distributed cache.\n",
      "25/11/27 14:56:48 WARN  yarn.Client: [Thread-9]: Same path resource file:///home/tsevero/.ivy2/jars/org.apache.ws.xmlschema_xmlschema-core-2.3.0.jar added multiple times to distributed cache.\n",
      "25/11/27 14:56:48 WARN  yarn.Client: [Thread-9]: Same path resource file:///home/tsevero/.ivy2/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.9.0.jar added multiple times to distributed cache.\n",
      "25/11/27 14:56:54 WARN  util.Utils: [Thread-9]: spark.executor.instances less than spark.dynamicAllocation.minExecutors is invalid, ignoring its setting, please update your configs.\n"
     ]
    }
   ],
   "source": [
    "def get_session(profile: str, dynamic_allocation_enabled: bool = True) -> utils.DBASparkAppSession:\n",
    "    \"\"\"Generates DBASparkAppSession.\"\"\"\n",
    "    \n",
    "    app_name = \"tsevero_cred_sql_validar\"\n",
    "    \n",
    "    spark_builder = (utils.DBASparkAppSession\n",
    "                     .builder\n",
    "                     .setAppName(app_name)\n",
    "                     .usingProcessProfile(profile)\n",
    "                    )\n",
    "    \n",
    "    if dynamic_allocation_enabled:\n",
    "        spark_builder.autoResourceManagement()\n",
    "\n",
    "    return spark_builder.build()\n",
    "\n",
    "session = get_session(profile='efd_t2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00777b24-69dc-4a6e-ba6f-4e11cd534937",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hive Session ID = 6b66bcb2-e1f8-445a-9ebd-76302b1e6fa7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|namespace         |\n",
      "+------------------+\n",
      "|anac              |\n",
      "|bcadastro         |\n",
      "|bpe               |\n",
      "|c115              |\n",
      "|ccc               |\n",
      "|ccg               |\n",
      "|cte               |\n",
      "|default           |\n",
      "|destda            |\n",
      "|detran_share      |\n",
      "|dime              |\n",
      "|due               |\n",
      "|efd               |\n",
      "|fci               |\n",
      "|gecob             |\n",
      "|gescol            |\n",
      "|gessimples        |\n",
      "|gplam             |\n",
      "|information_schema|\n",
      "|malhas            |\n",
      "+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "session.sparkSession.sql(\"SHOW DATABASES\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e6036a9-c4f4-4b3f-94b1-10b707e694ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üîç SISTEMA\n",
      "================================================================================\n",
      "Sess√£o Spark: tsevero_cred_sql_validar\n",
      "Vers√£o Spark: 3.5.4.3.5.7191000.0-30\n",
      "Iniciado em: 2025-11-27 14:57:12\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURA√á√ÉO INICIAL - \n",
    "# ============================================================================\n",
    "\n",
    "import sys\n",
    "import warnings\n",
    "from datetime import datetime, date\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# PySpark imports com aliases para evitar conflitos\n",
    "from pyspark.sql.functions import (\n",
    "    col as spark_col, \n",
    "    sum as spark_sum, \n",
    "    avg as spark_avg,\n",
    "    count as spark_count,\n",
    "    when as spark_when,\n",
    "    desc as spark_desc,\n",
    "    asc as spark_asc,\n",
    "    round as spark_round,\n",
    "    concat as spark_concat,\n",
    "    lit as spark_lit,\n",
    "    max as spark_max,\n",
    "    min as spark_min,\n",
    "    stddev as spark_stddev,\n",
    "    countDistinct as spark_countDistinct\n",
    ")\n",
    "from pyspark.sql.types import DoubleType, IntegerType\n",
    "\n",
    "# Configura√ß√µes de visualiza√ß√£o\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (16, 8)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "# ‚úÖ CORRE√á√ÉO: N√£o usar abs() que conflita com PySpark\n",
    "# pd.set_option('display.float_format', lambda x: f'{x:,.2f}' if abs(x) > 0.01 else f'{x:.6f}')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.precision', 2)\n",
    "\n",
    "# Acesso ao Spark\n",
    "spark = session.sparkSession\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"üîç SISTEMA\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Sess√£o Spark: {spark.sparkContext.appName}\")\n",
    "print(f\"Vers√£o Spark: {spark.version}\")\n",
    "print(f\"Iniciado em: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f81d4b6-9b05-49a6-882d-d51cec0cad8f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DIAGN√ìSTICO: DECLARA√á√ïES ZERADAS E OMISS√ïES\n",
      "DIME (Regime Normal) vs PGDAS (Simples Nacional)\n",
      "================================================================================\n",
      "\n",
      "============================================================\n",
      "1. ESTRUTURA DAS TABELAS\n",
      "============================================================\n",
      "\n",
      "üìã Regime Normal (DIME) (teste.cancel_zero_normal):\n",
      "   ‚úì periodos_omissos\n",
      "   ‚úì periodos_zerados\n",
      "   ‚úì periodos_sem_movimento\n",
      "   ‚úì total_periodos\n",
      "   ‚úì periodos_declarados\n",
      "   ‚úì qtde_indicios_fraude\n",
      "   ‚úì soma_score_indicios\n",
      "   ‚úì flag_omissao_dime_indicio\n",
      "   ‚úì flag_omissao_pgdas_indicio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   üìä Total de registros: 451,401\n",
      "\n",
      "üìã Simples Nacional (PGDAS) (teste.cancel_zero_simples):\n",
      "   ‚úì periodos_omissos\n",
      "   ‚úì periodos_zerados\n",
      "   ‚úì total_periodos\n",
      "   ‚úì periodos_declarados\n",
      "   ‚úì qtde_indicios_fraude\n",
      "   ‚úì soma_score_indicios\n",
      "   ‚úì flag_omissao_dime_indicio\n",
      "   ‚úì flag_omissao_pgdas_indicio\n",
      "   üìä Total de registros: 613,509\n",
      "\n",
      "üìã Tabela Principal Consolidada (teste.credito_dime_completo):\n",
      "   ‚úì valor_igual_total_periodos_60m\n",
      "   ‚úì primeiro_periodo_valor_igual_60m\n",
      "   ‚úì ultimo_periodo_valor_igual_60m\n",
      "   ‚úì valor_igual_total_periodos_12m\n",
      "   ‚úì primeiro_periodo_valor_igual_12m\n",
      "   ‚úì ultimo_periodo_valor_igual_12m\n",
      "   ‚úì periodo_inicial_sequencia_12m\n",
      "   ‚úì periodo_final_sequencia_12m\n",
      "   ‚úì periodos_omissos_normal\n",
      "   ‚úì periodos_zerados_normal\n",
      "   ‚úì total_periodos_normal\n",
      "   ‚úì periodos_declarados_normal\n",
      "   ‚úì qtde_indicios_normal\n",
      "   ‚úì soma_score_indicios_normal\n",
      "   ‚úì flag_omissao_dime_normal\n",
      "   ‚úì flag_omissao_pgdas_normal\n",
      "   ‚úì periodos_omissos_simples\n",
      "   ‚úì periodos_zerados_simples\n",
      "   ‚úì total_periodos_simples\n",
      "   ‚úì periodos_declarados_simples\n",
      "   ‚úì qtde_indicios_simples\n",
      "   ‚úì soma_score_indicios_simples\n",
      "   ‚úì flag_omissao_dime_simples\n",
      "   ‚úì flag_omissao_pgdas_simples\n",
      "   ‚úì qtde_indicios_fraude\n",
      "   ‚úì soma_score_indicios\n",
      "   ‚úì flag_empresa_suspeita\n",
      "   ‚úì flag_tem_pagamentos\n",
      "   ‚úì flag_tem_declaracoes_zeradas\n",
      "   ‚úì flag_tem_omissoes\n",
      "   üìä Total de registros: 54,777\n",
      "\n",
      "============================================================\n",
      "2. AN√ÅLISE: cancel_zero_normal (REGIME NORMAL - DIME)\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Resumo Geral:\n",
      "   Total de registros: 451,401\n",
      "   Total de CNPJs √∫nicos: 451,401\n",
      "\n",
      "üìã Per√≠odos:\n",
      "   Com per√≠odos omissos: 418,654 (92.7%)\n",
      "   Com per√≠odos zerados: 50,247 (11.1%)\n",
      "   Com sem movimento: 0 (0.0%)\n",
      "   M√©dia omissos: 47.54\n",
      "   M√©dia zerados: 1.07\n",
      "   M√°ximo omissos: 57\n",
      "   M√°ximo zerados: 57\n",
      "\n",
      "üîç Ind√≠cios:\n",
      "   Com ind√≠cios de fraude: 91,232\n",
      "   Flag omiss√£o DIME: 12,497\n",
      "   Flag omiss√£o PGDAS: 675 ‚ö†Ô∏è (DEVERIA SER 0 - NORMAL N√ÉO TEM PGDAS)\n",
      "   M√©dia de ind√≠cios: 1.85\n",
      "   M√©dia score: 189.45\n",
      "\n",
      "============================================================\n",
      "3. AN√ÅLISE: cancel_zero_simples (SIMPLES NACIONAL - PGDAS)\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Resumo Geral:\n",
      "   Total de registros: 613,509\n",
      "   Total de CNPJs √∫nicos: 613,509\n",
      "\n",
      "üìã Per√≠odos:\n",
      "   Com per√≠odos omissos: 510,468 (83.2%)\n",
      "   Com per√≠odos zerados: 145,416 (23.7%)\n",
      "   M√©dia omissos: 41.68\n",
      "   M√©dia zerados: 3.01\n",
      "   M√°ximo omissos: 57\n",
      "   M√°ximo zerados: 57\n",
      "\n",
      "üîç Ind√≠cios:\n",
      "   Com ind√≠cios de fraude: 323,074\n",
      "   Flag omiss√£o DIME: 1 ‚ö†Ô∏è (DEVERIA SER 0 - SIMPLES N√ÉO TEM DIME)\n",
      "   Flag omiss√£o PGDAS: 4,723\n",
      "   M√©dia de ind√≠cios: 3.32\n",
      "   M√©dia score: 356.42\n",
      "\n",
      "============================================================\n",
      "4. AN√ÅLISE: credito_dime_completo (TABELA PRINCIPAL)\n",
      "============================================================\n",
      "\n",
      "üìä Total de empresas: 54,777\n",
      "\n",
      "üî¥ FLAGS DE OMISS√ÉO:\n",
      "   flag_omissao_dime_normal: 3,430 ‚úÖ (CORRETO)\n",
      "   flag_omissao_dime_simples: 0 ‚ö†Ô∏è (INCORRETO - DIME N√ÉO TEM SIMPLES)\n",
      "   flag_omissao_pgdas_normal: 22 ‚ö†Ô∏è (INCORRETO - PGDAS N√ÉO TEM NORMAL)\n",
      "   flag_omissao_pgdas_simples: 82 ‚úÖ (CORRETO)\n",
      "\n",
      "üìã PER√çODOS - REGIME NORMAL:\n",
      "   Com zerados: 27,719 (m√©dia: 4.95)\n",
      "   Com omissos: 31,754 (m√©dia: 17.80)\n",
      "\n",
      "üìã PER√çODOS - SIMPLES NACIONAL:\n",
      "   Com zerados: 2,406 (m√©dia: 0.55)\n",
      "   Com omissos: 3,555 (m√©dia: 1.77)\n",
      "\n",
      "üîç IND√çCIOS:\n",
      "   M√©dia ind√≠cios Normal: 8.03 (score: 814.34)\n",
      "   M√©dia ind√≠cios Simples: 0.63 (score: 62.27)\n",
      "\n",
      "============================================================\n",
      "5. CRUZAMENTO: TABELAS DE ORIGEM vs DESTINO\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä CNPJs nas tabelas de origem:\n",
      "   cancel_zero_normal (com declara√ß√µes): 429,920\n",
      "   cancel_zero_simples (com declara√ß√µes): 547,284\n",
      "\n",
      "üìä CNPJs na tabela destino:\n",
      "   credito_dime_completo (com declara√ß√µes): 42,867\n",
      "\n",
      "‚úÖ Matches Normal:\n",
      "   38,906 / 429,920 (9.0%)\n",
      "\n",
      "‚úÖ Matches Simples:\n",
      "   3,961 / 547,284 (0.7%)\n",
      "\n",
      "============================================================\n",
      "6. IDENTIFICAR INCONSIST√äNCIAS\n",
      "============================================================\n",
      "\n",
      "üîç Verificando casos problem√°ticos:\n",
      "   ‚ö†Ô∏è Inconsist√™ncia 1: 862 empresas com flag_omissao_dime_normal=1 mas periodos_omissos_normal=0\n",
      "   ‚ö†Ô∏è Inconsist√™ncia 2: 29,186 empresas com periodos_omissos_normal>0 mas flag_omissao_dime_normal=0\n",
      "   ‚ö†Ô∏è Inconsist√™ncia 3: 22 empresas com flag_omissao_pgdas_normal=1 (INCORRETO)\n",
      "   ‚úÖ OK: Nenhuma flag_omissao_dime_simples=1 (correto, DIME s√≥ tem Normal)\n",
      "\n",
      "============================================================\n",
      "7. DISTRIBUI√á√ÉO POR FAIXAS DE PER√çODOS\n",
      "============================================================\n",
      "\n",
      "üìä Distribui√ß√£o por faixas:\n",
      "\n",
      "   Omissos Normal:\n",
      "          0:     23,023 empresas\n",
      "        1-3:        945 empresas\n",
      "        12+:     27,971 empresas\n",
      "        4-6:        907 empresas\n",
      "       7-11:      1,931 empresas\n",
      "\n",
      "   Omissos Simples:\n",
      "          0:     51,222 empresas\n",
      "        1-3:         33 empresas\n",
      "        12+:      3,307 empresas\n",
      "        4-6:         55 empresas\n",
      "       7-11:        160 empresas\n",
      "\n",
      "   Zerados Normal:\n",
      "          0:     27,058 empresas\n",
      "        1-3:     10,752 empresas\n",
      "        12+:      7,454 empresas\n",
      "        4-6:      4,987 empresas\n",
      "       7-11:      4,526 empresas\n",
      "\n",
      "   Zerados Simples:\n",
      "          0:     52,371 empresas\n",
      "        1-3:        679 empresas\n",
      "        12+:        929 empresas\n",
      "        4-6:        315 empresas\n",
      "       7-11:        483 empresas\n",
      "\n",
      "============================================================\n",
      "8. AMOSTRA DE EMPRESAS\n",
      "============================================================\n",
      "\n",
      "üîç Top 10 empresas com mais declara√ß√µes problem√°ticas:\n",
      "\n",
      "   ----------------------------------------------------------------------------------------------------------------------------------\n",
      "   CNPJ            Raz√£o Social                    Z_Nor  Z_Sim  O_Nor  O_Sim  Flags (DN|DS|PN|PS)\n",
      "   ----------------------------------------------------------------------------------------------------------------------------------\n",
      "   31865377000218  BICICLETE VEICULOS ELETRICO...      0      0      0     57                 0000\n",
      "   26603199000136  SENA BRASIL DESENVOLVIMENTO...      0      0     57      0                 0000\n",
      "   21303068000110  GRAN OESTE INDUSTRIA DE ALI...      0      0     57      0                 0000\n",
      "   26369790000170  VIVIANE CARDOSO ALVES               0      0     57      0                 0000\n",
      "   04040744000190  BENJAMIM BONASSI & FILHO LTDA       0      0     57      0                 0000\n",
      "   23632495000177  23.632.495 FELIPE ROZO MORAES       0      0      0     57                 0001\n",
      "   10841535000149  REI DO PREGO IMPORTACAO E C...      0      0     57      0                 0000\n",
      "   09496060000200  LEONI MULLER                        0      0      0     57                 0000\n",
      "   02272175000183  AUTO POSTO NOVA ERECHIM LTDA        0      0     57      0                 0000\n",
      "   26614392000334  LEGEND BRASIL LTDA                  0      0      0     57                 0000\n",
      "   ----------------------------------------------------------------------------------------------------------------------------------\n",
      "   Legenda: Z_Nor=Zerados Normal | Z_Sim=Zerados Simples | O_Nor=Omissos Normal | O_Sim=Omissos Simples\n",
      "   Flags: DN=DIME Normal | DS=DIME Simples | PN=PGDAS Normal | PS=PGDAS Simples\n",
      "\n",
      "============================================================\n",
      "9. RESUMO E RECOMENDA√á√ïES\n",
      "============================================================\n",
      "\n",
      "üìã ENTENDIMENTO CONCEITUAL CORRETO:\n",
      "   ‚Ä¢ DIME = Declara√ß√£o do Regime NORMAL (n√£o-Simples)\n",
      "   ‚Ä¢ PGDAS = Declara√ß√£o do Regime SIMPLES NACIONAL\n",
      "   \n",
      "üéØ AN√ÅLISE DE NOMENCLATURA DAS FLAGS:\n",
      "   \n",
      "   Situa√ß√£o ATUAL na tabela credito_dime_completo:\n",
      "   ‚úÖ flag_omissao_dime_normal   ‚Üí CORRETO (DIME √© do Normal)\n",
      "   ‚ùå flag_omissao_dime_simples  ‚Üí INCORRETO (DIME n√£o existe no Simples)\n",
      "   ‚ùå flag_omissao_pgdas_normal  ‚Üí INCORRETO (PGDAS n√£o existe no Normal)\n",
      "   ‚úÖ flag_omissao_pgdas_simples ‚Üí CORRETO (PGDAS √© do Simples)\n",
      "   \n",
      "üí° NOMENCLATURA CORRETA DEVERIA SER:\n",
      "   Op√ß√£o 1 (Simplificada - RECOMENDADA):\n",
      "      ‚Ä¢ flag_omissao_dime   (remove \"_normal\" pois √© redundante)\n",
      "      ‚Ä¢ flag_omissao_pgdas  (remove \"_simples\" pois √© redundante)\n",
      "   \n",
      "   Op√ß√£o 2 (Manter padr√£o atual, corrigindo os erros):\n",
      "      ‚Ä¢ flag_omissao_dime_normal   (mant√©m)\n",
      "      ‚Ä¢ flag_omissao_pgdas_simples (mant√©m)\n",
      "      ‚Ä¢ REMOVER: flag_omissao_dime_simples\n",
      "      ‚Ä¢ REMOVER: flag_omissao_pgdas_normal\n",
      "\n",
      "üìä COLUNAS DE PER√çODOS (estas est√£o corretas):\n",
      "   ‚úÖ periodos_zerados_normal   ‚Üí Per√≠odos zerados do Regime Normal (DIME)\n",
      "   ‚úÖ periodos_omissos_normal   ‚Üí Per√≠odos omissos do Regime Normal (DIME)\n",
      "   ‚úÖ periodos_zerados_simples  ‚Üí Per√≠odos zerados do Simples Nacional (PGDAS)\n",
      "   ‚úÖ periodos_omissos_simples  ‚Üí Per√≠odos omissos do Simples Nacional (PGDAS)\n",
      "\n",
      "üîß A√á√ïES RECOMENDADAS NO SQL:\n",
      "   1. Remover as colunas flag_omissao_dime_simples e flag_omissao_pgdas_normal\n",
      "   2. Renomear flag_omissao_dime_normal para flag_omissao_dime\n",
      "   3. Renomear flag_omissao_pgdas_simples para flag_omissao_pgdas\n",
      "   4. Verificar se as tabelas de origem tamb√©m t√™m essa inconsist√™ncia\n",
      "\n",
      "\n",
      "================================================================================\n",
      "FIM DO DIAGN√ìSTICO\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DIAGN√ìSTICO COMPLETO: DECLARA√á√ïES ZERADAS E OMISS√ïES\n",
    "Baseado nas tabelas: cancel_zero_normal, cancel_zero_simples e credito_dime_completo\n",
    "DIME (Regime Normal) vs PGDAS (Simples Nacional)\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"DIAGN√ìSTICO: DECLARA√á√ïES ZERADAS E OMISS√ïES\")\n",
    "print(\"DIME (Regime Normal) vs PGDAS (Simples Nacional)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ============================================================================\n",
    "# 1. VERIFICAR ESTRUTURA DAS TABELAS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"1. ESTRUTURA DAS TABELAS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "tabelas = {\n",
    "    'teste.cancel_zero_normal': 'Regime Normal (DIME)',\n",
    "    'teste.cancel_zero_simples': 'Simples Nacional (PGDAS)',\n",
    "    'teste.credito_dime_completo': 'Tabela Principal Consolidada'\n",
    "}\n",
    "\n",
    "for tabela, descricao in tabelas.items():\n",
    "    try:\n",
    "        print(f\"\\nüìã {descricao} ({tabela}):\")\n",
    "        colunas = spark.sql(f\"DESCRIBE {tabela}\").collect()\n",
    "        \n",
    "        # Filtrar colunas relevantes\n",
    "        colunas_rel = [col.col_name for col in colunas if any(x in col.col_name.lower() \n",
    "                       for x in ['periodo', 'omiss', 'zerad', 'flag', 'indicio'])]\n",
    "        \n",
    "        if colunas_rel:\n",
    "            for col in colunas_rel:\n",
    "                print(f\"   ‚úì {col}\")\n",
    "        else:\n",
    "            print(f\"   ‚ö†Ô∏è Nenhuma coluna de declara√ß√£o encontrada\")\n",
    "            \n",
    "        # Contar total de registros\n",
    "        total = spark.sql(f\"SELECT COUNT(*) as total FROM {tabela}\").collect()[0]['total']\n",
    "        print(f\"   üìä Total de registros: {total:,}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Erro ao acessar tabela: {str(e)[:100]}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 2. AN√ÅLISE DETALHADA: cancel_zero_normal (REGIME NORMAL - DIME)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"2. AN√ÅLISE: cancel_zero_normal (REGIME NORMAL - DIME)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    stats_normal = spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            COUNT(*) as total_registros,\n",
    "            COUNT(DISTINCT nu_cnpj) as total_cnpjs,\n",
    "            \n",
    "            -- Per√≠odos Omissos e Zerados\n",
    "            SUM(CASE WHEN periodos_omissos > 0 THEN 1 ELSE 0 END) as com_omissos,\n",
    "            SUM(CASE WHEN periodos_zerados > 0 THEN 1 ELSE 0 END) as com_zerados,\n",
    "            SUM(CASE WHEN periodos_sem_movimento > 0 THEN 1 ELSE 0 END) as com_sem_movimento,\n",
    "            \n",
    "            ROUND(AVG(periodos_omissos), 2) as media_omissos,\n",
    "            ROUND(AVG(periodos_zerados), 2) as media_zerados,\n",
    "            ROUND(AVG(periodos_sem_movimento), 2) as media_sem_movimento,\n",
    "            \n",
    "            MAX(periodos_omissos) as max_omissos,\n",
    "            MAX(periodos_zerados) as max_zerados,\n",
    "            \n",
    "            -- Ind√≠cios de Fraude\n",
    "            SUM(CASE WHEN qtde_indicios_fraude > 0 THEN 1 ELSE 0 END) as com_indicios,\n",
    "            SUM(CASE WHEN flag_omissao_dime_indicio = 1 THEN 1 ELSE 0 END) as flag_dime,\n",
    "            SUM(CASE WHEN flag_omissao_pgdas_indicio = 1 THEN 1 ELSE 0 END) as flag_pgdas,\n",
    "            \n",
    "            ROUND(AVG(qtde_indicios_fraude), 2) as media_indicios,\n",
    "            ROUND(AVG(soma_score_indicios), 2) as media_score_indicios\n",
    "        FROM teste.cancel_zero_normal\n",
    "    \"\"\").collect()[0]\n",
    "    \n",
    "    print(f\"\\nüìä Resumo Geral:\")\n",
    "    print(f\"   Total de registros: {stats_normal['total_registros']:,}\")\n",
    "    print(f\"   Total de CNPJs √∫nicos: {stats_normal['total_cnpjs']:,}\")\n",
    "    \n",
    "    print(f\"\\nüìã Per√≠odos:\")\n",
    "    print(f\"   Com per√≠odos omissos: {stats_normal['com_omissos']:,} ({stats_normal['com_omissos']/stats_normal['total_registros']*100:.1f}%)\")\n",
    "    print(f\"   Com per√≠odos zerados: {stats_normal['com_zerados']:,} ({stats_normal['com_zerados']/stats_normal['total_registros']*100:.1f}%)\")\n",
    "    print(f\"   Com sem movimento: {stats_normal['com_sem_movimento']:,} ({stats_normal['com_sem_movimento']/stats_normal['total_registros']*100:.1f}%)\")\n",
    "    print(f\"   M√©dia omissos: {stats_normal['media_omissos']:.2f}\")\n",
    "    print(f\"   M√©dia zerados: {stats_normal['media_zerados']:.2f}\")\n",
    "    print(f\"   M√°ximo omissos: {stats_normal['max_omissos']:,}\")\n",
    "    print(f\"   M√°ximo zerados: {stats_normal['max_zerados']:,}\")\n",
    "    \n",
    "    print(f\"\\nüîç Ind√≠cios:\")\n",
    "    print(f\"   Com ind√≠cios de fraude: {stats_normal['com_indicios']:,}\")\n",
    "    print(f\"   Flag omiss√£o DIME: {stats_normal['flag_dime']:,}\")\n",
    "    print(f\"   Flag omiss√£o PGDAS: {stats_normal['flag_pgdas']:,} ‚ö†Ô∏è (DEVERIA SER 0 - NORMAL N√ÉO TEM PGDAS)\")\n",
    "    print(f\"   M√©dia de ind√≠cios: {stats_normal['media_indicios']:.2f}\")\n",
    "    print(f\"   M√©dia score: {stats_normal['media_score_indicios']:.2f}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Erro: {str(e)[:300]}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 3. AN√ÅLISE DETALHADA: cancel_zero_simples (SIMPLES NACIONAL - PGDAS)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"3. AN√ÅLISE: cancel_zero_simples (SIMPLES NACIONAL - PGDAS)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    stats_simples = spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            COUNT(*) as total_registros,\n",
    "            COUNT(DISTINCT nu_cnpj) as total_cnpjs,\n",
    "            \n",
    "            -- Per√≠odos Omissos e Zerados\n",
    "            SUM(CASE WHEN periodos_omissos > 0 THEN 1 ELSE 0 END) as com_omissos,\n",
    "            SUM(CASE WHEN periodos_zerados > 0 THEN 1 ELSE 0 END) as com_zerados,\n",
    "            \n",
    "            ROUND(AVG(periodos_omissos), 2) as media_omissos,\n",
    "            ROUND(AVG(periodos_zerados), 2) as media_zerados,\n",
    "            \n",
    "            MAX(periodos_omissos) as max_omissos,\n",
    "            MAX(periodos_zerados) as max_zerados,\n",
    "            \n",
    "            -- Ind√≠cios de Fraude\n",
    "            SUM(CASE WHEN qtde_indicios_fraude > 0 THEN 1 ELSE 0 END) as com_indicios,\n",
    "            SUM(CASE WHEN flag_omissao_dime_indicio = 1 THEN 1 ELSE 0 END) as flag_dime,\n",
    "            SUM(CASE WHEN flag_omissao_pgdas_indicio = 1 THEN 1 ELSE 0 END) as flag_pgdas,\n",
    "            \n",
    "            ROUND(AVG(qtde_indicios_fraude), 2) as media_indicios,\n",
    "            ROUND(AVG(soma_score_indicios), 2) as media_score_indicios\n",
    "        FROM teste.cancel_zero_simples\n",
    "    \"\"\").collect()[0]\n",
    "    \n",
    "    print(f\"\\nüìä Resumo Geral:\")\n",
    "    print(f\"   Total de registros: {stats_simples['total_registros']:,}\")\n",
    "    print(f\"   Total de CNPJs √∫nicos: {stats_simples['total_cnpjs']:,}\")\n",
    "    \n",
    "    print(f\"\\nüìã Per√≠odos:\")\n",
    "    print(f\"   Com per√≠odos omissos: {stats_simples['com_omissos']:,} ({stats_simples['com_omissos']/stats_simples['total_registros']*100:.1f}%)\")\n",
    "    print(f\"   Com per√≠odos zerados: {stats_simples['com_zerados']:,} ({stats_simples['com_zerados']/stats_simples['total_registros']*100:.1f}%)\")\n",
    "    print(f\"   M√©dia omissos: {stats_simples['media_omissos']:.2f}\")\n",
    "    print(f\"   M√©dia zerados: {stats_simples['media_zerados']:.2f}\")\n",
    "    print(f\"   M√°ximo omissos: {stats_simples['max_omissos']:,}\")\n",
    "    print(f\"   M√°ximo zerados: {stats_simples['max_zerados']:,}\")\n",
    "    \n",
    "    print(f\"\\nüîç Ind√≠cios:\")\n",
    "    print(f\"   Com ind√≠cios de fraude: {stats_simples['com_indicios']:,}\")\n",
    "    print(f\"   Flag omiss√£o DIME: {stats_simples['flag_dime']:,} ‚ö†Ô∏è (DEVERIA SER 0 - SIMPLES N√ÉO TEM DIME)\")\n",
    "    print(f\"   Flag omiss√£o PGDAS: {stats_simples['flag_pgdas']:,}\")\n",
    "    print(f\"   M√©dia de ind√≠cios: {stats_simples['media_indicios']:.2f}\")\n",
    "    print(f\"   M√©dia score: {stats_simples['media_score_indicios']:.2f}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Erro: {str(e)[:300]}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 4. AN√ÅLISE: credito_dime_completo (TABELA PRINCIPAL)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"4. AN√ÅLISE: credito_dime_completo (TABELA PRINCIPAL)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    stats_completo = spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            COUNT(*) as total_empresas,\n",
    "            \n",
    "            -- Flags\n",
    "            SUM(CASE WHEN flag_omissao_dime_normal = 1 THEN 1 ELSE 0 END) as flag_omissao_dime_normal,\n",
    "            SUM(CASE WHEN flag_omissao_dime_simples = 1 THEN 1 ELSE 0 END) as flag_omissao_dime_simples,\n",
    "            SUM(CASE WHEN flag_omissao_pgdas_normal = 1 THEN 1 ELSE 0 END) as flag_omissao_pgdas_normal,\n",
    "            SUM(CASE WHEN flag_omissao_pgdas_simples = 1 THEN 1 ELSE 0 END) as flag_omissao_pgdas_simples,\n",
    "            \n",
    "            -- Per√≠odos Normal\n",
    "            SUM(CASE WHEN periodos_zerados_normal > 0 THEN 1 ELSE 0 END) as com_zerados_normal,\n",
    "            SUM(CASE WHEN periodos_omissos_normal > 0 THEN 1 ELSE 0 END) as com_omissos_normal,\n",
    "            ROUND(AVG(periodos_zerados_normal), 2) as media_zerados_normal,\n",
    "            ROUND(AVG(periodos_omissos_normal), 2) as media_omissos_normal,\n",
    "            \n",
    "            -- Per√≠odos Simples\n",
    "            SUM(CASE WHEN periodos_zerados_simples > 0 THEN 1 ELSE 0 END) as com_zerados_simples,\n",
    "            SUM(CASE WHEN periodos_omissos_simples > 0 THEN 1 ELSE 0 END) as com_omissos_simples,\n",
    "            ROUND(AVG(periodos_zerados_simples), 2) as media_zerados_simples,\n",
    "            ROUND(AVG(periodos_omissos_simples), 2) as media_omissos_simples,\n",
    "            \n",
    "            -- Ind√≠cios\n",
    "            ROUND(AVG(qtde_indicios_normal), 2) as media_indicios_normal,\n",
    "            ROUND(AVG(qtde_indicios_simples), 2) as media_indicios_simples,\n",
    "            ROUND(AVG(soma_score_indicios_normal), 2) as media_score_normal,\n",
    "            ROUND(AVG(soma_score_indicios_simples), 2) as media_score_simples\n",
    "        FROM teste.credito_dime_completo\n",
    "    \"\"\").collect()[0]\n",
    "    \n",
    "    print(f\"\\nüìä Total de empresas: {stats_completo['total_empresas']:,}\")\n",
    "    \n",
    "    print(f\"\\nüî¥ FLAGS DE OMISS√ÉO:\")\n",
    "    print(f\"   flag_omissao_dime_normal: {stats_completo['flag_omissao_dime_normal']:,} ‚úÖ (CORRETO)\")\n",
    "    print(f\"   flag_omissao_dime_simples: {stats_completo['flag_omissao_dime_simples']:,} ‚ö†Ô∏è (INCORRETO - DIME N√ÉO TEM SIMPLES)\")\n",
    "    print(f\"   flag_omissao_pgdas_normal: {stats_completo['flag_omissao_pgdas_normal']:,} ‚ö†Ô∏è (INCORRETO - PGDAS N√ÉO TEM NORMAL)\")\n",
    "    print(f\"   flag_omissao_pgdas_simples: {stats_completo['flag_omissao_pgdas_simples']:,} ‚úÖ (CORRETO)\")\n",
    "    \n",
    "    print(f\"\\nüìã PER√çODOS - REGIME NORMAL:\")\n",
    "    print(f\"   Com zerados: {stats_completo['com_zerados_normal']:,} (m√©dia: {stats_completo['media_zerados_normal']:.2f})\")\n",
    "    print(f\"   Com omissos: {stats_completo['com_omissos_normal']:,} (m√©dia: {stats_completo['media_omissos_normal']:.2f})\")\n",
    "    \n",
    "    print(f\"\\nüìã PER√çODOS - SIMPLES NACIONAL:\")\n",
    "    print(f\"   Com zerados: {stats_completo['com_zerados_simples']:,} (m√©dia: {stats_completo['media_zerados_simples']:.2f})\")\n",
    "    print(f\"   Com omissos: {stats_completo['com_omissos_simples']:,} (m√©dia: {stats_completo['media_omissos_simples']:.2f})\")\n",
    "    \n",
    "    print(f\"\\nüîç IND√çCIOS:\")\n",
    "    print(f\"   M√©dia ind√≠cios Normal: {stats_completo['media_indicios_normal']:.2f} (score: {stats_completo['media_score_normal']:.2f})\")\n",
    "    print(f\"   M√©dia ind√≠cios Simples: {stats_completo['media_indicios_simples']:.2f} (score: {stats_completo['media_score_simples']:.2f})\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Erro: {str(e)[:300]}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 5. CRUZAMENTO DE DADOS: ORIGEM vs DESTINO\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"5. CRUZAMENTO: TABELAS DE ORIGEM vs DESTINO\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    cruzamento = spark.sql(\"\"\"\n",
    "        WITH origem_normal AS (\n",
    "            SELECT DISTINCT REGEXP_REPLACE(TRIM(CAST(nu_cnpj AS STRING)), '[^0-9]', '') AS cnpj_limpo\n",
    "            FROM teste.cancel_zero_normal\n",
    "            WHERE periodos_zerados > 0 OR periodos_omissos > 0\n",
    "        ),\n",
    "        origem_simples AS (\n",
    "            SELECT DISTINCT REGEXP_REPLACE(TRIM(CAST(nu_cnpj AS STRING)), '[^0-9]', '') AS cnpj_limpo\n",
    "            FROM teste.cancel_zero_simples\n",
    "            WHERE periodos_zerados > 0 OR periodos_omissos > 0\n",
    "        ),\n",
    "        destino AS (\n",
    "            SELECT DISTINCT REGEXP_REPLACE(TRIM(CAST(nu_cnpj AS STRING)), '[^0-9]', '') AS cnpj_limpo\n",
    "            FROM teste.credito_dime_completo\n",
    "            WHERE periodos_zerados_normal > 0 \n",
    "               OR periodos_zerados_simples > 0\n",
    "               OR periodos_omissos_normal > 0\n",
    "               OR periodos_omissos_simples > 0\n",
    "        )\n",
    "        SELECT \n",
    "            (SELECT COUNT(*) FROM origem_normal) as origem_normal_cnpjs,\n",
    "            (SELECT COUNT(*) FROM origem_simples) as origem_simples_cnpjs,\n",
    "            (SELECT COUNT(*) FROM destino) as destino_cnpjs,\n",
    "            (SELECT COUNT(*) FROM origem_normal o WHERE EXISTS (SELECT 1 FROM destino d WHERE d.cnpj_limpo = o.cnpj_limpo)) as match_normal,\n",
    "            (SELECT COUNT(*) FROM origem_simples o WHERE EXISTS (SELECT 1 FROM destino d WHERE d.cnpj_limpo = o.cnpj_limpo)) as match_simples\n",
    "    \"\"\").collect()[0]\n",
    "    \n",
    "    print(f\"\\nüìä CNPJs nas tabelas de origem:\")\n",
    "    print(f\"   cancel_zero_normal (com declara√ß√µes): {cruzamento['origem_normal_cnpjs']:,}\")\n",
    "    print(f\"   cancel_zero_simples (com declara√ß√µes): {cruzamento['origem_simples_cnpjs']:,}\")\n",
    "    \n",
    "    print(f\"\\nüìä CNPJs na tabela destino:\")\n",
    "    print(f\"   credito_dime_completo (com declara√ß√µes): {cruzamento['destino_cnpjs']:,}\")\n",
    "    \n",
    "    if cruzamento['origem_normal_cnpjs'] > 0:\n",
    "        perc_normal = cruzamento['match_normal'] / cruzamento['origem_normal_cnpjs'] * 100\n",
    "        print(f\"\\n‚úÖ Matches Normal:\")\n",
    "        print(f\"   {cruzamento['match_normal']:,} / {cruzamento['origem_normal_cnpjs']:,} ({perc_normal:.1f}%)\")\n",
    "    \n",
    "    if cruzamento['origem_simples_cnpjs'] > 0:\n",
    "        perc_simples = cruzamento['match_simples'] / cruzamento['origem_simples_cnpjs'] * 100\n",
    "        print(f\"\\n‚úÖ Matches Simples:\")\n",
    "        print(f\"   {cruzamento['match_simples']:,} / {cruzamento['origem_simples_cnpjs']:,} ({perc_simples:.1f}%)\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Erro: {str(e)[:300]}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 6. IDENTIFICAR INCONSIST√äNCIAS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"6. IDENTIFICAR INCONSIST√äNCIAS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nüîç Verificando casos problem√°ticos:\")\n",
    "\n",
    "# Inconsist√™ncia 1: Flag mas sem per√≠odos\n",
    "try:\n",
    "    incon1 = spark.sql(\"\"\"\n",
    "        SELECT COUNT(*) as qtd\n",
    "        FROM teste.credito_dime_completo\n",
    "        WHERE flag_omissao_dime_normal = 1 \n",
    "          AND (periodos_omissos_normal IS NULL OR periodos_omissos_normal = 0)\n",
    "    \"\"\").collect()[0]['qtd']\n",
    "    \n",
    "    if incon1 > 0:\n",
    "        print(f\"   ‚ö†Ô∏è Inconsist√™ncia 1: {incon1:,} empresas com flag_omissao_dime_normal=1 mas periodos_omissos_normal=0\")\n",
    "    else:\n",
    "        print(f\"   ‚úÖ OK: Flag omiss√£o DIME Normal consistente com per√≠odos omissos\")\n",
    "except:\n",
    "    print(f\"   ‚ö†Ô∏è N√£o foi poss√≠vel verificar inconsist√™ncia 1\")\n",
    "\n",
    "# Inconsist√™ncia 2: Per√≠odos mas sem flag\n",
    "try:\n",
    "    incon2 = spark.sql(\"\"\"\n",
    "        SELECT COUNT(*) as qtd\n",
    "        FROM teste.credito_dime_completo\n",
    "        WHERE periodos_omissos_normal > 0\n",
    "          AND (flag_omissao_dime_normal IS NULL OR flag_omissao_dime_normal = 0)\n",
    "    \"\"\").collect()[0]['qtd']\n",
    "    \n",
    "    if incon2 > 0:\n",
    "        print(f\"   ‚ö†Ô∏è Inconsist√™ncia 2: {incon2:,} empresas com periodos_omissos_normal>0 mas flag_omissao_dime_normal=0\")\n",
    "    else:\n",
    "        print(f\"   ‚úÖ OK: Per√≠odos omissos Normal consistentes com flag\")\n",
    "except:\n",
    "    print(f\"   ‚ö†Ô∏è N√£o foi poss√≠vel verificar inconsist√™ncia 2\")\n",
    "\n",
    "# Inconsist√™ncia 3: Flags com nomenclatura errada\n",
    "try:\n",
    "    incon3 = spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            SUM(CASE WHEN flag_omissao_pgdas_normal = 1 THEN 1 ELSE 0 END) as pgdas_normal,\n",
    "            SUM(CASE WHEN flag_omissao_dime_simples = 1 THEN 1 ELSE 0 END) as dime_simples\n",
    "        FROM teste.credito_dime_completo\n",
    "    \"\"\").collect()[0]\n",
    "    \n",
    "    if incon3['pgdas_normal'] > 0:\n",
    "        print(f\"   ‚ö†Ô∏è Inconsist√™ncia 3: {incon3['pgdas_normal']:,} empresas com flag_omissao_pgdas_normal=1 (INCORRETO)\")\n",
    "    else:\n",
    "        print(f\"   ‚úÖ OK: Nenhuma flag_omissao_pgdas_normal=1 (correto, PGDAS s√≥ tem Simples)\")\n",
    "        \n",
    "    if incon3['dime_simples'] > 0:\n",
    "        print(f\"   ‚ö†Ô∏è Inconsist√™ncia 4: {incon3['dime_simples']:,} empresas com flag_omissao_dime_simples=1 (INCORRETO)\")\n",
    "    else:\n",
    "        print(f\"   ‚úÖ OK: Nenhuma flag_omissao_dime_simples=1 (correto, DIME s√≥ tem Normal)\")\n",
    "except:\n",
    "    print(f\"   ‚ö†Ô∏è N√£o foi poss√≠vel verificar inconsist√™ncias 3 e 4\")\n",
    "\n",
    "# ============================================================================\n",
    "# 7. DISTRIBUI√á√ÉO POR FAIXAS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"7. DISTRIBUI√á√ÉO POR FAIXAS DE PER√çODOS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    dist = spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            'Zerados Normal' as tipo,\n",
    "            CASE \n",
    "                WHEN periodos_zerados_normal = 0 THEN '0'\n",
    "                WHEN periodos_zerados_normal BETWEEN 1 AND 3 THEN '1-3'\n",
    "                WHEN periodos_zerados_normal BETWEEN 4 AND 6 THEN '4-6'\n",
    "                WHEN periodos_zerados_normal BETWEEN 7 AND 11 THEN '7-11'\n",
    "                ELSE '12+'\n",
    "            END as faixa,\n",
    "            COUNT(*) as qtd\n",
    "        FROM teste.credito_dime_completo\n",
    "        GROUP BY tipo, faixa\n",
    "        \n",
    "        UNION ALL\n",
    "        \n",
    "        SELECT \n",
    "            'Omissos Normal' as tipo,\n",
    "            CASE \n",
    "                WHEN periodos_omissos_normal = 0 THEN '0'\n",
    "                WHEN periodos_omissos_normal BETWEEN 1 AND 3 THEN '1-3'\n",
    "                WHEN periodos_omissos_normal BETWEEN 4 AND 6 THEN '4-6'\n",
    "                WHEN periodos_omissos_normal BETWEEN 7 AND 11 THEN '7-11'\n",
    "                ELSE '12+'\n",
    "            END as faixa,\n",
    "            COUNT(*) as qtd\n",
    "        FROM teste.credito_dime_completo\n",
    "        GROUP BY tipo, faixa\n",
    "        \n",
    "        UNION ALL\n",
    "        \n",
    "        SELECT \n",
    "            'Zerados Simples' as tipo,\n",
    "            CASE \n",
    "                WHEN periodos_zerados_simples = 0 THEN '0'\n",
    "                WHEN periodos_zerados_simples BETWEEN 1 AND 3 THEN '1-3'\n",
    "                WHEN periodos_zerados_simples BETWEEN 4 AND 6 THEN '4-6'\n",
    "                WHEN periodos_zerados_simples BETWEEN 7 AND 11 THEN '7-11'\n",
    "                ELSE '12+'\n",
    "            END as faixa,\n",
    "            COUNT(*) as qtd\n",
    "        FROM teste.credito_dime_completo\n",
    "        GROUP BY tipo, faixa\n",
    "        \n",
    "        UNION ALL\n",
    "        \n",
    "        SELECT \n",
    "            'Omissos Simples' as tipo,\n",
    "            CASE \n",
    "                WHEN periodos_omissos_simples = 0 THEN '0'\n",
    "                WHEN periodos_omissos_simples BETWEEN 1 AND 3 THEN '1-3'\n",
    "                WHEN periodos_omissos_simples BETWEEN 4 AND 6 THEN '4-6'\n",
    "                WHEN periodos_omissos_simples BETWEEN 7 AND 11 THEN '7-11'\n",
    "                ELSE '12+'\n",
    "            END as faixa,\n",
    "            COUNT(*) as qtd\n",
    "        FROM teste.credito_dime_completo\n",
    "        GROUP BY tipo, faixa\n",
    "        ORDER BY tipo, faixa\n",
    "    \"\"\").toPandas()\n",
    "    \n",
    "    print(\"\\nüìä Distribui√ß√£o por faixas:\")\n",
    "    for tipo in dist['tipo'].unique():\n",
    "        print(f\"\\n   {tipo}:\")\n",
    "        df_tipo = dist[dist['tipo'] == tipo]\n",
    "        for _, row in df_tipo.iterrows():\n",
    "            print(f\"      {row['faixa']:>5}: {int(row['qtd']):>10,} empresas\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Erro: {str(e)[:300]}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 8. AMOSTRA DE DADOS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"8. AMOSTRA DE EMPRESAS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nüîç Top 10 empresas com mais declara√ß√µes problem√°ticas:\")\n",
    "\n",
    "try:\n",
    "    amostra = spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            nu_cnpj,\n",
    "            nm_razao_social,\n",
    "            periodos_zerados_normal,\n",
    "            periodos_zerados_simples,\n",
    "            periodos_omissos_normal,\n",
    "            periodos_omissos_simples,\n",
    "            flag_omissao_dime_normal,\n",
    "            flag_omissao_dime_simples,\n",
    "            flag_omissao_pgdas_normal,\n",
    "            flag_omissao_pgdas_simples,\n",
    "            (periodos_zerados_normal + periodos_zerados_simples + \n",
    "             periodos_omissos_normal + periodos_omissos_simples) as total\n",
    "        FROM teste.credito_dime_completo\n",
    "        WHERE (periodos_zerados_normal + periodos_zerados_simples + \n",
    "               periodos_omissos_normal + periodos_omissos_simples) > 0\n",
    "        ORDER BY total DESC\n",
    "        LIMIT 10\n",
    "    \"\"\").toPandas()\n",
    "    \n",
    "    print(\"\\n   \" + \"-\" * 130)\n",
    "    print(f\"   {'CNPJ':<15} {'Raz√£o Social':<30} {'Z_Nor':>6} {'Z_Sim':>6} {'O_Nor':>6} {'O_Sim':>6} {'Flags (DN|DS|PN|PS)':>20}\")\n",
    "    print(\"   \" + \"-\" * 130)\n",
    "    for _, row in amostra.iterrows():\n",
    "        flags = f\"{int(row['flag_omissao_dime_normal'] or 0)}{int(row['flag_omissao_dime_simples'] or 0)}{int(row['flag_omissao_pgdas_normal'] or 0)}{int(row['flag_omissao_pgdas_simples'] or 0)}\"\n",
    "        razao = (row['nm_razao_social'][:27] + '...') if len(str(row['nm_razao_social'])) > 30 else str(row['nm_razao_social'])\n",
    "        print(f\"   {row['nu_cnpj']:<15} {razao:<30} {int(row['periodos_zerados_normal']):>6} {int(row['periodos_zerados_simples']):>6} {int(row['periodos_omissos_normal']):>6} {int(row['periodos_omissos_simples']):>6} {flags:>20}\")\n",
    "    print(\"   \" + \"-\" * 130)\n",
    "    print(\"   Legenda: Z_Nor=Zerados Normal | Z_Sim=Zerados Simples | O_Nor=Omissos Normal | O_Sim=Omissos Simples\")\n",
    "    print(\"   Flags: DN=DIME Normal | DS=DIME Simples | PN=PGDAS Normal | PS=PGDAS Simples\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Erro: {str(e)[:300]}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 9. RESUMO E RECOMENDA√á√ïES\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"9. RESUMO E RECOMENDA√á√ïES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\"\"\n",
    "üìã ENTENDIMENTO CONCEITUAL CORRETO:\n",
    "   ‚Ä¢ DIME = Declara√ß√£o do Regime NORMAL (n√£o-Simples)\n",
    "   ‚Ä¢ PGDAS = Declara√ß√£o do Regime SIMPLES NACIONAL\n",
    "   \n",
    "üéØ AN√ÅLISE DE NOMENCLATURA DAS FLAGS:\n",
    "   \n",
    "   Situa√ß√£o ATUAL na tabela credito_dime_completo:\n",
    "   ‚úÖ flag_omissao_dime_normal   ‚Üí CORRETO (DIME √© do Normal)\n",
    "   ‚ùå flag_omissao_dime_simples  ‚Üí INCORRETO (DIME n√£o existe no Simples)\n",
    "   ‚ùå flag_omissao_pgdas_normal  ‚Üí INCORRETO (PGDAS n√£o existe no Normal)\n",
    "   ‚úÖ flag_omissao_pgdas_simples ‚Üí CORRETO (PGDAS √© do Simples)\n",
    "   \n",
    "üí° NOMENCLATURA CORRETA DEVERIA SER:\n",
    "   Op√ß√£o 1 (Simplificada - RECOMENDADA):\n",
    "      ‚Ä¢ flag_omissao_dime   (remove \"_normal\" pois √© redundante)\n",
    "      ‚Ä¢ flag_omissao_pgdas  (remove \"_simples\" pois √© redundante)\n",
    "   \n",
    "   Op√ß√£o 2 (Manter padr√£o atual, corrigindo os erros):\n",
    "      ‚Ä¢ flag_omissao_dime_normal   (mant√©m)\n",
    "      ‚Ä¢ flag_omissao_pgdas_simples (mant√©m)\n",
    "      ‚Ä¢ REMOVER: flag_omissao_dime_simples\n",
    "      ‚Ä¢ REMOVER: flag_omissao_pgdas_normal\n",
    "\n",
    "üìä COLUNAS DE PER√çODOS (estas est√£o corretas):\n",
    "   ‚úÖ periodos_zerados_normal   ‚Üí Per√≠odos zerados do Regime Normal (DIME)\n",
    "   ‚úÖ periodos_omissos_normal   ‚Üí Per√≠odos omissos do Regime Normal (DIME)\n",
    "   ‚úÖ periodos_zerados_simples  ‚Üí Per√≠odos zerados do Simples Nacional (PGDAS)\n",
    "   ‚úÖ periodos_omissos_simples  ‚Üí Per√≠odos omissos do Simples Nacional (PGDAS)\n",
    "\n",
    "üîß A√á√ïES RECOMENDADAS NO SQL:\n",
    "   1. Remover as colunas flag_omissao_dime_simples e flag_omissao_pgdas_normal\n",
    "   2. Renomear flag_omissao_dime_normal para flag_omissao_dime\n",
    "   3. Renomear flag_omissao_pgdas_simples para flag_omissao_pgdas\n",
    "   4. Verificar se as tabelas de origem tamb√©m t√™m essa inconsist√™ncia\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FIM DO DIAGN√ìSTICO\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f94139fe-b19e-4831-b854-667b0c6bc1a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DIAGN√ìSTICO: TABELAS FONTES ORIGINAIS\n",
      "================================================================================\n",
      "\n",
      "============================================================\n",
      "1. FONTE: usr_sat_ods.ods_decl_dime_raw (DECLARA√á√ïES DIME)\n",
      "============================================================\n",
      "\n",
      "üìã Estrutura da tabela:\n",
      "   ‚úì nu_cnpj\n",
      "   ‚úì nu_cnpj_grupo\n",
      "   ‚úì nu_per_ref\n",
      "   ‚úì vl_faturamento\n",
      "   ‚úì vl_cred_mes_anterior\n",
      "   ‚úì vl_cred_auc\n",
      "   ‚úì vl_cred_dcip\n",
      "   ‚úì vl_tot_cred\n",
      "   ‚úì vl_tot_deb\n",
      "   ‚úì vl_cred_ap_cons\n",
      "   ‚úì vl_deb_ap_cons\n",
      "   ‚úì vl_cred_mes_seguinte\n",
      "   ‚úì vl_deb_recolher\n",
      "   ‚úì vl_sdo_cred_exportacao\n",
      "   ‚úì vl_sdo_cred_isenta\n",
      "   ‚úì vl_sdo_cred_diferida\n",
      "   ‚úì vl_sdo_cred_outros\n",
      "   ‚úì vl_receita_bruta\n",
      "   ‚úì vl_tot_ent_imposto_creditado\n",
      "   ‚úì vl_tot_sai_imposto_creditado\n",
      "   ‚úì vl_faturamento_comunicacao\n",
      "\n",
      "üìä Estat√≠sticas Gerais (per√≠odo 202101-202509):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Total de registros: 4,428,586\n",
      "   Total de CNPJs √∫nicos: 165,616\n",
      "   Primeiro per√≠odo: 202101\n",
      "   √öltimo per√≠odo: 202509\n",
      "   Per√≠odos distintos: 57\n",
      "\n",
      "üîç An√°lise de Declara√ß√µes Zeradas:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Apenas faturamento = 0: 457,812 (10.3%)\n",
      "   Cr√©dito E D√©bito = 0: 0 (0.0%)\n",
      "   Crit√©rio ATUAL (Fat=0 OU Cred=Deb=0): 457,812 (10.3%)\n",
      "   Sem movimento total: 0 (0.0%)\n",
      "   Faturamento NULL: 1,633,194\n",
      "   Cr√©dito NULL: 1,994,781\n",
      "   D√©bito NULL: 2,185,371\n",
      "\n",
      "üìÖ Registros por per√≠odo (√∫ltimos 12 meses):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   202509:  92,742 registros |  92,742 CNPJs |  9,876 zerados\n",
      "   202508:  92,888 registros |  92,888 CNPJs | 10,077 zerados\n",
      "   202507:  94,497 registros |  94,497 CNPJs | 10,300 zerados\n",
      "   202506:  94,589 registros |  94,589 CNPJs | 10,284 zerados\n",
      "   202505:  96,137 registros |  96,137 CNPJs | 10,542 zerados\n",
      "   202504:  96,628 registros |  96,628 CNPJs | 10,423 zerados\n",
      "   202503:  97,353 registros |  97,353 CNPJs | 10,475 zerados\n",
      "   202502:  99,718 registros |  99,718 CNPJs | 10,842 zerados\n",
      "   202501: 100,514 registros | 100,514 CNPJs | 10,969 zerados\n",
      "   202412:  83,907 registros |  83,907 CNPJs |  9,773 zerados\n",
      "   202411:  83,685 registros |  83,685 CNPJs |  9,788 zerados\n",
      "   202410:  87,043 registros |  87,043 CNPJs |  9,869 zerados\n",
      "   202409:  86,850 registros |  86,850 CNPJs |  9,862 zerados\n",
      "\n",
      "üî¨ Amostra de 5 declara√ß√µes consideradas ZERADAS:\n",
      "\n",
      "   --------------------------------------------------------------------------------------------------------------\n",
      "   CNPJ            Per√≠odo       Rec.Bruta  Faturamento      Cr√©dito       D√©bito      Deb.Rec\n",
      "   --------------------------------------------------------------------------------------------------------------\n",
      "   ‚ùå Erro: unsupported format string passed to NoneType.__format__\n",
      "\n",
      "============================================================\n",
      "2. FONTE: usr_sat_ods.vw_sna_pgdasd_grupo_empresarial (PGDAS)\n",
      "============================================================\n",
      "\n",
      "üìã Estrutura da tabela:\n",
      "   ‚úì nu_cnpj_grupo\n",
      "   ‚úì nu_cnpj_matriz\n",
      "   ‚úì nu_per_ref\n",
      "   ‚úì vl_rec_bruta\n",
      "   ‚úì vl_rec_bruta_op_internas\n",
      "   ‚úì vl_rec_bruta_op_externas\n",
      "   ‚úì vl_icms_sc\n",
      "   ‚úì vl_rec_bruta_em_12m\n",
      "   ‚úì vl_rec_bruta_no_ano\n",
      "   ‚úì vl_venda_st_icms\n",
      "   ‚úì vl_venda_industrializ_st_icms\n",
      "   ‚úì vl_prest_serv_com_st_icms\n",
      "   ‚úì vl_prest_serv_transp_st_icms\n",
      "\n",
      "üìä Estat√≠sticas Gerais (per√≠odo 202101-202509):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Total de registros: 17,874,580\n",
      "   Total de CNPJs √∫nicos: 561,806\n",
      "   Primeiro per√≠odo: 202101\n",
      "   √öltimo per√≠odo: 202509\n",
      "   Per√≠odos distintos: 57\n",
      "\n",
      "üîç An√°lise de Declara√ß√µes Zeradas:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Receita Bruta = 0: 4,335,621 (24.3%)\n",
      "   Receita Bruta NULL: 0\n",
      "   ICMS SC = 0: 11,121,897 (62.2%)\n",
      "   ICMS SC NULL: 0\n",
      "\n",
      "üìÖ Registros por per√≠odo (√∫ltimos 12 meses):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   202509: 367,444 registros | 367,444 CNPJs | 80,135 zerados\n",
      "   202508: 366,717 registros | 366,717 CNPJs | 82,412 zerados\n",
      "   202507: 364,842 registros | 364,842 CNPJs | 81,329 zerados\n",
      "   202506: 361,783 registros | 361,783 CNPJs | 82,255 zerados\n",
      "   202505: 359,492 registros | 359,492 CNPJs | 81,015 zerados\n",
      "   202504: 356,729 registros | 356,729 CNPJs | 80,420 zerados\n",
      "   202503: 353,605 registros | 353,605 CNPJs | 79,545 zerados\n",
      "   202502: 351,217 registros | 351,217 CNPJs | 81,462 zerados\n",
      "   202501: 347,182 registros | 347,182 CNPJs | 83,591 zerados\n",
      "   202412: 352,038 registros | 352,038 CNPJs | 82,523 zerados\n",
      "   202411: 350,496 registros | 350,496 CNPJs | 82,275 zerados\n",
      "   202410: 348,130 registros | 348,130 CNPJs | 79,045 zerados\n",
      "   202409: 344,701 registros | 344,701 CNPJs | 79,466 zerados\n",
      "\n",
      "üî¨ Amostra de 5 declara√ß√µes consideradas ZERADAS:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   ----------------------------------------------------------------------------------------------------\n",
      "   CNPJ            Per√≠odo       Rec.Bruta      Rec.12m      Rec.Ano      ICMS SC\n",
      "   ----------------------------------------------------------------------------------------------------\n",
      "   59614521000181  202506             0.00   226,101.00    75,367.00         0.00\n",
      "   54888786000154  202505             0.00         0.00         0.00         0.00\n",
      "   01749482000140  202506             0.00         0.00         0.00         0.00\n",
      "   56195631000168  202506             0.00    50,529.60    19,008.00         0.00\n",
      "   39793762000128  202506             0.00     6,021.00         0.00         0.00\n",
      "   ----------------------------------------------------------------------------------------------------\n",
      "\n",
      "============================================================\n",
      "3. FONTE: usr_sat_ods.vw_cad_contrib (CADASTRO)\n",
      "============================================================\n",
      "\n",
      "üìä Distribui√ß√£o por Regime de Apura√ß√£o:\n",
      "   ‚ùå Erro: [CANNOT_UP_CAST_DATATYPE] Cannot up cast contrib.nu_cpf from \"STRING\" to \"BIGINT\".\n",
      "The type path of the target object is:\n",
      "\n",
      "You can either add an explicit cast to the input data or choose a higher precision type of the field in the target object\n",
      "\n",
      "============================================================\n",
      "4. FONTE: neaf.empresa_indicio (IND√çCIOS DE FRAUDE)\n",
      "============================================================\n",
      "\n",
      "üìä Estat√≠sticas Gerais:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Total de registros: 4,160,088\n",
      "   Total de CNPJs √∫nicos: 568,557\n",
      "   Ind√≠cios distintos: 101\n",
      "   Ind√≠cios atuais (cd_atual=1): 3,320,546\n",
      "   Ind√≠cios SC (cd_uf='SC'): 3,939,235\n",
      "\n",
      "üìã Distribui√ß√£o por C√≥digo de Ind√≠cio (SC, atuais):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Ind√≠cio  20:  369,921 ocorr√™ncias |  369,921 CNPJs | score m√©dio: 100.00\n",
      "   Ind√≠cio  52:  301,407 ocorr√™ncias |  301,407 CNPJs | score m√©dio: 100.00\n",
      "   Ind√≠cio  85:  205,507 ocorr√™ncias |  205,507 CNPJs | score m√©dio: 0.00\n",
      "   Ind√≠cio  33:  201,835 ocorr√™ncias |  201,835 CNPJs | score m√©dio: 120.00\n",
      "   Ind√≠cio  39:  178,888 ocorr√™ncias |  178,888 CNPJs | score m√©dio: 120.00\n",
      "   Ind√≠cio  32:  164,949 ocorr√™ncias |  164,949 CNPJs | score m√©dio: 120.00\n",
      "   Ind√≠cio  18:  161,540 ocorr√™ncias |  161,540 CNPJs | score m√©dio: 120.00\n",
      "   Ind√≠cio  27:  147,164 ocorr√™ncias |  147,164 CNPJs | score m√©dio: 50.00\n",
      "   Ind√≠cio  78:  142,707 ocorr√™ncias |  142,707 CNPJs | score m√©dio: 100.00\n",
      "   Ind√≠cio  30:  111,456 ocorr√™ncias |  111,456 CNPJs | score m√©dio: 120.00\n",
      "   Ind√≠cio  45:   96,742 ocorr√™ncias |   96,742 CNPJs | score m√©dio: 300.00\n",
      "   Ind√≠cio  75:   81,008 ocorr√™ncias |   81,008 CNPJs | score m√©dio: 100.00\n",
      "   Ind√≠cio  23:   65,900 ocorr√™ncias |   65,900 CNPJs | score m√©dio: 50.00\n",
      "   Ind√≠cio  19:   59,731 ocorr√™ncias |   59,731 CNPJs | score m√©dio: 120.00\n",
      "   Ind√≠cio  21:   46,008 ocorr√™ncias |   46,008 CNPJs | score m√©dio: 120.00\n",
      "   Ind√≠cio  76:   45,773 ocorr√™ncias |   45,773 CNPJs | score m√©dio: 100.00\n",
      "   Ind√≠cio  24:   43,586 ocorr√™ncias |   43,586 CNPJs | score m√©dio: 50.00\n",
      "   Ind√≠cio  73:   40,175 ocorr√™ncias |   40,175 CNPJs | score m√©dio: 100.00\n",
      "   Ind√≠cio  64:   38,078 ocorr√™ncias |   38,078 CNPJs | score m√©dio: 200.00\n",
      "   Ind√≠cio  38:   32,246 ocorr√™ncias |   32,246 CNPJs | score m√©dio: 120.00\n",
      "\n",
      "üîç An√°lise dos Ind√≠cios 11 (DIME) e 12 (PGDAS):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Ind√≠cio 12 (PGDAS (omiss√£o)):\n",
      "      Ocorr√™ncias: 6,005\n",
      "      CNPJs √∫nicos: 6,005\n",
      "      Score m√©dio: 81.30\n",
      "      Score range: 50.00 - 120.00\n",
      "   Ind√≠cio 11 (DIME (omiss√£o)):\n",
      "      Ocorr√™ncias: 13,160\n",
      "      CNPJs √∫nicos: 13,160\n",
      "      Score m√©dio: 98.20\n",
      "      Score range: 50.00 - 120.00\n",
      "\n",
      "============================================================\n",
      "5. VALIDA√á√ÉO: MATRIZ DE PER√çODOS (202101-202509)\n",
      "============================================================\n",
      "\n",
      "üìÖ Per√≠odos esperados: 57 per√≠odos (202101 a 202509)\n",
      "   2021: 12 meses | 2022: 12 meses | 2023: 12 meses | 2024: 12 meses | 2025: 9 meses\n",
      "\n",
      "üîç Verificando cobertura de per√≠odos na DIME:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Todos os 57 per√≠odos presentes na DIME\n",
      "   üìä Total de per√≠odos encontrados: 57\n",
      "\n",
      "üîç Verificando cobertura de per√≠odos no PGDAS:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Todos os 57 per√≠odos presentes no PGDAS\n",
      "   üìä Total de per√≠odos encontrados: 57\n",
      "\n",
      "============================================================\n",
      "6. AN√ÅLISE DE OMISS√ïES: VALIDA√á√ÉO DA L√ìGICA\n",
      "============================================================\n",
      "\n",
      "üîç Testando a l√≥gica de c√°lculo de omiss√µes:\n",
      "   (Pegando 3 CNPJs aleat√≥rios e calculando suas omiss√µes)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   üìä CNPJ: 37127086000128\n",
      "      Total de per√≠odos: 57\n",
      "      Per√≠odos declarados: 1\n",
      "      Per√≠odos omissos: 56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 192:==============================================>        (17 + 3) / 20]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      √öltimas 5 declara√ß√µes:\n",
      "   ‚ùå Erro: unsupported format string passed to NoneType.__format__\n",
      "\n",
      "============================================================\n",
      "7. RESUMO E CONCLUS√ïES\n",
      "============================================================\n",
      "\n",
      "üéØ PRINCIPAIS ACHADOS:\n",
      "\n",
      "1Ô∏è‚É£ CRIT√âRIO DE DECLARA√á√ÉO ZERADA (DIME):\n",
      "   ‚Ä¢ Crit√©rio atual: VL_FATURAMENTO = 0 OU (VL_TOT_CRED = 0 AND VL_TOT_DEB = 0)\n",
      "   ‚Ä¢ Este crit√©rio est√° CORRETO e captura declara√ß√µes com \"sem movimento\"\n",
      "   \n",
      "2Ô∏è‚É£ CRIT√âRIO DE DECLARA√á√ÉO ZERADA (PGDAS):\n",
      "   ‚Ä¢ Crit√©rio atual: vl_rec_bruta = 0\n",
      "   ‚Ä¢ Crit√©rio simples e direto, est√° CORRETO\n",
      "   \n",
      "3Ô∏è‚É£ MATRIZ DE PER√çODOS:\n",
      "   ‚Ä¢ Verificar se todos os 57 per√≠odos (202101-202509) est√£o presentes\n",
      "   ‚Ä¢ Validar se h√° \"buracos\" na s√©rie temporal\n",
      "   \n",
      "4Ô∏è‚É£ IND√çCIOS DE FRAUDE:\n",
      "   ‚Ä¢ Ind√≠cio 11 = omiss√£o DIME (Regime Normal)\n",
      "   ‚Ä¢ Ind√≠cio 12 = omiss√£o PGDAS (Simples Nacional)\n",
      "   ‚Ä¢ Verificar consist√™ncia entre as flags e os ind√≠cios\n",
      "   \n",
      "5Ô∏è‚É£ REGIME DE APURA√á√ÉO:\n",
      "   ‚Ä¢ Filtro por regime ATUAL est√° correto\n",
      "   ‚Ä¢ Empresas podem ter mudado de regime ao longo do tempo\n",
      "   \n",
      "‚ö†Ô∏è POSS√çVEIS PROBLEMAS A INVESTIGAR:\n",
      "\n",
      "1. Se h√° per√≠odos faltantes nas fontes originais\n",
      "2. Se a l√≥gica de \"omiss√£o\" est√° considerando per√≠odos corretos (57 vs 8?)\n",
      "3. Se h√° CNPJs duplicados ou com formatos diferentes\n",
      "4. Se os filtros de cd_uf='SC' e cd_atual=1 est√£o corretos para ind√≠cios\n",
      "\n",
      "üí° PR√ìXIMOS PASSOS:\n",
      "   ‚Ä¢ Com base nos resultados acima, ajustar os SQLs se necess√°rio\n",
      "   ‚Ä¢ Validar especialmente a contagem de per√≠odos (57 per√≠odos esperados)\n",
      "   ‚Ä¢ Verificar se \"total_periodos\" nas tabelas auxiliares est√° correto\n",
      "\n",
      "\n",
      "================================================================================\n",
      "FIM DO DIAGN√ìSTICO DAS FONTES ORIGINAIS\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DIAGN√ìSTICO DAS TABELAS FONTES ORIGINAIS\n",
    "Valida√ß√£o de dados de entrada para cancel_zero_normal e cancel_zero_simples\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"DIAGN√ìSTICO: TABELAS FONTES ORIGINAIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ============================================================================\n",
    "# 1. AN√ÅLISE DA FONTE: ods_decl_dime_raw (DECLARA√á√ïES DIME - REGIME NORMAL)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"1. FONTE: usr_sat_ods.ods_decl_dime_raw (DECLARA√á√ïES DIME)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    # Verificar estrutura\n",
    "    print(\"\\nüìã Estrutura da tabela:\")\n",
    "    colunas_dime = spark.sql(\"DESCRIBE usr_sat_ods.ods_decl_dime_raw\").collect()\n",
    "    colunas_importantes = [col.col_name for col in colunas_dime if any(x in col.col_name.upper() \n",
    "                          for x in ['CNPJ', 'PER_REF', 'FATURAMENTO', 'RECEITA', 'CRED', 'DEB'])]\n",
    "    for col in colunas_importantes:\n",
    "        print(f\"   ‚úì {col}\")\n",
    "    \n",
    "    # Estat√≠sticas gerais\n",
    "    print(\"\\nüìä Estat√≠sticas Gerais (per√≠odo 202101-202509):\")\n",
    "    stats_dime = spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            COUNT(*) as total_registros,\n",
    "            COUNT(DISTINCT nu_cnpj) as total_cnpjs,\n",
    "            MIN(nu_per_ref) as primeiro_periodo,\n",
    "            MAX(nu_per_ref) as ultimo_periodo,\n",
    "            COUNT(DISTINCT nu_per_ref) as qtde_periodos_distintos\n",
    "        FROM usr_sat_ods.ods_decl_dime_raw\n",
    "        WHERE nu_per_ref BETWEEN 202101 AND 202509\n",
    "    \"\"\").collect()[0]\n",
    "    \n",
    "    print(f\"   Total de registros: {stats_dime['total_registros']:,}\")\n",
    "    print(f\"   Total de CNPJs √∫nicos: {stats_dime['total_cnpjs']:,}\")\n",
    "    print(f\"   Primeiro per√≠odo: {stats_dime['primeiro_periodo']}\")\n",
    "    print(f\"   √öltimo per√≠odo: {stats_dime['ultimo_periodo']}\")\n",
    "    print(f\"   Per√≠odos distintos: {stats_dime['qtde_periodos_distintos']}\")\n",
    "    \n",
    "    # An√°lise de declara√ß√µes zeradas\n",
    "    print(\"\\nüîç An√°lise de Declara√ß√µes Zeradas:\")\n",
    "    zeradas_dime = spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            -- Crit√©rio ATUAL do SQL\n",
    "            SUM(CASE WHEN VL_FATURAMENTO = 0 THEN 1 ELSE 0 END) as apenas_faturamento_zero,\n",
    "            SUM(CASE WHEN VL_TOT_CRED = 0 AND VL_TOT_DEB = 0 THEN 1 ELSE 0 END) as cred_deb_zero,\n",
    "            SUM(CASE WHEN VL_FATURAMENTO = 0 OR (VL_TOT_CRED = 0 AND VL_TOT_DEB = 0) THEN 1 ELSE 0 END) as criterio_atual,\n",
    "            \n",
    "            -- Crit√©rio ALTERNATIVO (mais restritivo)\n",
    "            SUM(CASE WHEN VL_RECEITA_BRUTA = 0 AND VL_FATURAMENTO = 0 \n",
    "                          AND VL_TOT_CRED = 0 AND VL_TOT_DEB = 0 \n",
    "                          AND VL_DEB_RECOLHER = 0 THEN 1 ELSE 0 END) as sem_movimento_total,\n",
    "            \n",
    "            -- An√°lise de campos NULL\n",
    "            SUM(CASE WHEN VL_FATURAMENTO IS NULL THEN 1 ELSE 0 END) as faturamento_null,\n",
    "            SUM(CASE WHEN VL_TOT_CRED IS NULL THEN 1 ELSE 0 END) as cred_null,\n",
    "            SUM(CASE WHEN VL_TOT_DEB IS NULL THEN 1 ELSE 0 END) as deb_null,\n",
    "            \n",
    "            COUNT(*) as total\n",
    "        FROM usr_sat_ods.ods_decl_dime_raw\n",
    "        WHERE nu_per_ref BETWEEN 202101 AND 202509\n",
    "    \"\"\").collect()[0]\n",
    "    \n",
    "    print(f\"   Apenas faturamento = 0: {zeradas_dime['apenas_faturamento_zero']:,} ({zeradas_dime['apenas_faturamento_zero']/zeradas_dime['total']*100:.1f}%)\")\n",
    "    print(f\"   Cr√©dito E D√©bito = 0: {zeradas_dime['cred_deb_zero']:,} ({zeradas_dime['cred_deb_zero']/zeradas_dime['total']*100:.1f}%)\")\n",
    "    print(f\"   Crit√©rio ATUAL (Fat=0 OU Cred=Deb=0): {zeradas_dime['criterio_atual']:,} ({zeradas_dime['criterio_atual']/zeradas_dime['total']*100:.1f}%)\")\n",
    "    print(f\"   Sem movimento total: {zeradas_dime['sem_movimento_total']:,} ({zeradas_dime['sem_movimento_total']/zeradas_dime['total']*100:.1f}%)\")\n",
    "    print(f\"   Faturamento NULL: {zeradas_dime['faturamento_null']:,}\")\n",
    "    print(f\"   Cr√©dito NULL: {zeradas_dime['cred_null']:,}\")\n",
    "    print(f\"   D√©bito NULL: {zeradas_dime['deb_null']:,}\")\n",
    "    \n",
    "    # Distribui√ß√£o por per√≠odo\n",
    "    print(\"\\nüìÖ Registros por per√≠odo (√∫ltimos 12 meses):\")\n",
    "    dist_periodo = spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            nu_per_ref as periodo,\n",
    "            COUNT(*) as qtd_registros,\n",
    "            COUNT(DISTINCT nu_cnpj) as qtd_cnpjs,\n",
    "            SUM(CASE WHEN VL_FATURAMENTO = 0 OR (VL_TOT_CRED = 0 AND VL_TOT_DEB = 0) THEN 1 ELSE 0 END) as qtd_zerados\n",
    "        FROM usr_sat_ods.ods_decl_dime_raw\n",
    "        WHERE nu_per_ref BETWEEN 202409 AND 202509\n",
    "        GROUP BY nu_per_ref\n",
    "        ORDER BY nu_per_ref DESC\n",
    "    \"\"\").toPandas()\n",
    "    \n",
    "    for _, row in dist_periodo.iterrows():\n",
    "        print(f\"   {row['periodo']}: {int(row['qtd_registros']):>7,} registros | {int(row['qtd_cnpjs']):>7,} CNPJs | {int(row['qtd_zerados']):>6,} zerados\")\n",
    "    \n",
    "    # Amostra de declara√ß√µes zeradas\n",
    "    print(\"\\nüî¨ Amostra de 5 declara√ß√µes consideradas ZERADAS:\")\n",
    "    amostra_zeradas = spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            nu_cnpj,\n",
    "            nu_per_ref,\n",
    "            VL_RECEITA_BRUTA,\n",
    "            VL_FATURAMENTO,\n",
    "            VL_TOT_CRED,\n",
    "            VL_TOT_DEB,\n",
    "            VL_DEB_RECOLHER\n",
    "        FROM usr_sat_ods.ods_decl_dime_raw\n",
    "        WHERE nu_per_ref BETWEEN 202101 AND 202509\n",
    "          AND (VL_FATURAMENTO = 0 OR (VL_TOT_CRED = 0 AND VL_TOT_DEB = 0))\n",
    "        LIMIT 5\n",
    "    \"\"\").toPandas()\n",
    "    \n",
    "    print(\"\\n   \" + \"-\" * 110)\n",
    "    print(f\"   {'CNPJ':<15} {'Per√≠odo':<10} {'Rec.Bruta':>12} {'Faturamento':>12} {'Cr√©dito':>12} {'D√©bito':>12} {'Deb.Rec':>12}\")\n",
    "    print(\"   \" + \"-\" * 110)\n",
    "    for _, row in amostra_zeradas.iterrows():\n",
    "        print(f\"   {row['nu_cnpj']:<15} {row['nu_per_ref']:<10} {row['VL_RECEITA_BRUTA']:>12,.2f} {row['VL_FATURAMENTO']:>12,.2f} {row['VL_TOT_CRED']:>12,.2f} {row['VL_TOT_DEB']:>12,.2f} {row['VL_DEB_RECOLHER']:>12,.2f}\")\n",
    "    print(\"   \" + \"-\" * 110)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Erro: {str(e)[:300]}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 2. AN√ÅLISE DA FONTE: vw_sna_pgdasd_grupo_empresarial (PGDAS - SIMPLES)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"2. FONTE: usr_sat_ods.vw_sna_pgdasd_grupo_empresarial (PGDAS)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    # Verificar estrutura\n",
    "    print(\"\\nüìã Estrutura da tabela:\")\n",
    "    colunas_pgdas = spark.sql(\"DESCRIBE usr_sat_ods.vw_sna_pgdasd_grupo_empresarial\").collect()\n",
    "    colunas_importantes = [col.col_name for col in colunas_pgdas if any(x in col.col_name.lower() \n",
    "                          for x in ['cnpj', 'per_ref', 'rec_bruta', 'icms'])]\n",
    "    for col in colunas_importantes:\n",
    "        print(f\"   ‚úì {col}\")\n",
    "    \n",
    "    # Estat√≠sticas gerais\n",
    "    print(\"\\nüìä Estat√≠sticas Gerais (per√≠odo 202101-202509):\")\n",
    "    stats_pgdas = spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            COUNT(*) as total_registros,\n",
    "            COUNT(DISTINCT nu_cnpj_matriz) as total_cnpjs,\n",
    "            MIN(nu_per_ref) as primeiro_periodo,\n",
    "            MAX(nu_per_ref) as ultimo_periodo,\n",
    "            COUNT(DISTINCT nu_per_ref) as qtde_periodos_distintos\n",
    "        FROM usr_sat_ods.vw_sna_pgdasd_grupo_empresarial\n",
    "        WHERE nu_per_ref BETWEEN 202101 AND 202509\n",
    "    \"\"\").collect()[0]\n",
    "    \n",
    "    print(f\"   Total de registros: {stats_pgdas['total_registros']:,}\")\n",
    "    print(f\"   Total de CNPJs √∫nicos: {stats_pgdas['total_cnpjs']:,}\")\n",
    "    print(f\"   Primeiro per√≠odo: {stats_pgdas['primeiro_periodo']}\")\n",
    "    print(f\"   √öltimo per√≠odo: {stats_pgdas['ultimo_periodo']}\")\n",
    "    print(f\"   Per√≠odos distintos: {stats_pgdas['qtde_periodos_distintos']}\")\n",
    "    \n",
    "    # An√°lise de declara√ß√µes zeradas\n",
    "    print(\"\\nüîç An√°lise de Declara√ß√µes Zeradas:\")\n",
    "    zeradas_pgdas = spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            SUM(CASE WHEN vl_rec_bruta = 0 THEN 1 ELSE 0 END) as rec_bruta_zero,\n",
    "            SUM(CASE WHEN vl_rec_bruta IS NULL THEN 1 ELSE 0 END) as rec_bruta_null,\n",
    "            SUM(CASE WHEN vl_icms_sc = 0 THEN 1 ELSE 0 END) as icms_zero,\n",
    "            SUM(CASE WHEN vl_icms_sc IS NULL THEN 1 ELSE 0 END) as icms_null,\n",
    "            COUNT(*) as total\n",
    "        FROM usr_sat_ods.vw_sna_pgdasd_grupo_empresarial\n",
    "        WHERE nu_per_ref BETWEEN 202101 AND 202509\n",
    "    \"\"\").collect()[0]\n",
    "    \n",
    "    print(f\"   Receita Bruta = 0: {zeradas_pgdas['rec_bruta_zero']:,} ({zeradas_pgdas['rec_bruta_zero']/zeradas_pgdas['total']*100:.1f}%)\")\n",
    "    print(f\"   Receita Bruta NULL: {zeradas_pgdas['rec_bruta_null']:,}\")\n",
    "    print(f\"   ICMS SC = 0: {zeradas_pgdas['icms_zero']:,} ({zeradas_pgdas['icms_zero']/zeradas_pgdas['total']*100:.1f}%)\")\n",
    "    print(f\"   ICMS SC NULL: {zeradas_pgdas['icms_null']:,}\")\n",
    "    \n",
    "    # Distribui√ß√£o por per√≠odo\n",
    "    print(\"\\nüìÖ Registros por per√≠odo (√∫ltimos 12 meses):\")\n",
    "    dist_periodo_pgdas = spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            nu_per_ref as periodo,\n",
    "            COUNT(*) as qtd_registros,\n",
    "            COUNT(DISTINCT nu_cnpj_matriz) as qtd_cnpjs,\n",
    "            SUM(CASE WHEN vl_rec_bruta = 0 THEN 1 ELSE 0 END) as qtd_zerados\n",
    "        FROM usr_sat_ods.vw_sna_pgdasd_grupo_empresarial\n",
    "        WHERE nu_per_ref BETWEEN 202409 AND 202509\n",
    "        GROUP BY nu_per_ref\n",
    "        ORDER BY nu_per_ref DESC\n",
    "    \"\"\").toPandas()\n",
    "    \n",
    "    for _, row in dist_periodo_pgdas.iterrows():\n",
    "        print(f\"   {row['periodo']}: {int(row['qtd_registros']):>7,} registros | {int(row['qtd_cnpjs']):>7,} CNPJs | {int(row['qtd_zerados']):>6,} zerados\")\n",
    "    \n",
    "    # Amostra de declara√ß√µes zeradas\n",
    "    print(\"\\nüî¨ Amostra de 5 declara√ß√µes consideradas ZERADAS:\")\n",
    "    amostra_zeradas_pgdas = spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            nu_cnpj_matriz,\n",
    "            nu_per_ref,\n",
    "            vl_rec_bruta,\n",
    "            vl_rec_bruta_em_12m,\n",
    "            vl_rec_bruta_no_ano,\n",
    "            vl_icms_sc\n",
    "        FROM usr_sat_ods.vw_sna_pgdasd_grupo_empresarial\n",
    "        WHERE nu_per_ref BETWEEN 202101 AND 202509\n",
    "          AND vl_rec_bruta = 0\n",
    "        LIMIT 5\n",
    "    \"\"\").toPandas()\n",
    "    \n",
    "    print(\"\\n   \" + \"-\" * 100)\n",
    "    print(f\"   {'CNPJ':<15} {'Per√≠odo':<10} {'Rec.Bruta':>12} {'Rec.12m':>12} {'Rec.Ano':>12} {'ICMS SC':>12}\")\n",
    "    print(\"   \" + \"-\" * 100)\n",
    "    for _, row in amostra_zeradas_pgdas.iterrows():\n",
    "        print(f\"   {row['nu_cnpj_matriz']:<15} {row['nu_per_ref']:<10} {row['vl_rec_bruta']:>12,.2f} {row['vl_rec_bruta_em_12m']:>12,.2f} {row['vl_rec_bruta_no_ano']:>12,.2f} {row['vl_icms_sc']:>12,.2f}\")\n",
    "    print(\"   \" + \"-\" * 100)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Erro: {str(e)[:300]}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 3. AN√ÅLISE: vw_cad_contrib (CADASTRO DE CONTRIBUINTES)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"3. FONTE: usr_sat_ods.vw_cad_contrib (CADASTRO)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    # Estat√≠sticas de regime\n",
    "    print(\"\\nüìä Distribui√ß√£o por Regime de Apura√ß√£o:\")\n",
    "    regimes = spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            nm_reg_apuracao,\n",
    "            COUNT(*) as qtd\n",
    "        FROM usr_sat_ods.vw_cad_contrib\n",
    "        GROUP BY nm_reg_apuracao\n",
    "        ORDER BY qtd DESC\n",
    "    \"\"\").toPandas()\n",
    "    \n",
    "    total_cadastro = regimes['qtd'].sum()\n",
    "    for _, row in regimes.iterrows():\n",
    "        regime = row['nm_reg_apuracao'] if row['nm_reg_apuracao'] else 'NULL'\n",
    "        print(f\"   {regime:<30}: {int(row['qtd']):>10,} ({row['qtd']/total_cadastro*100:.1f}%)\")\n",
    "    \n",
    "    # Verificar se h√° CNPJs que mudaram de regime\n",
    "    print(\"\\nüîÑ Verificar mudan√ßas de regime (amostra):\")\n",
    "    print(\"   (Verificando se a l√≥gica de filtrar por regime ATUAL est√° correta)\")\n",
    "    \n",
    "    mudancas = spark.sql(\"\"\"\n",
    "        WITH regime_atual AS (\n",
    "            SELECT \n",
    "                nu_cnpj,\n",
    "                nm_reg_apuracao,\n",
    "                dt_inicio_atividade\n",
    "            FROM usr_sat_ods.vw_cad_contrib\n",
    "            WHERE nm_reg_apuracao IS NOT NULL\n",
    "        )\n",
    "        SELECT \n",
    "            nm_reg_apuracao,\n",
    "            COUNT(*) as qtd,\n",
    "            MIN(dt_inicio_atividade) as dt_mais_antiga,\n",
    "            MAX(dt_inicio_atividade) as dt_mais_recente\n",
    "        FROM regime_atual\n",
    "        GROUP BY nm_reg_apuracao\n",
    "    \"\"\").toPandas()\n",
    "    \n",
    "    for _, row in mudancas.iterrows():\n",
    "        regime = row['nm_reg_apuracao'] if row['nm_reg_apuracao'] else 'NULL'\n",
    "        print(f\"   {regime}: {int(row['qtd']):,} empresas\")\n",
    "        if row['dt_mais_antiga'] and row['dt_mais_recente']:\n",
    "            print(f\"      Per√≠odo de in√≠cio: {row['dt_mais_antiga']} a {row['dt_mais_recente']}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Erro: {str(e)[:300]}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 4. AN√ÅLISE: neaf.empresa_indicio (IND√çCIOS DE FRAUDE)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"4. FONTE: neaf.empresa_indicio (IND√çCIOS DE FRAUDE)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    # Estat√≠sticas gerais\n",
    "    print(\"\\nüìä Estat√≠sticas Gerais:\")\n",
    "    stats_indicios = spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            COUNT(*) as total_registros,\n",
    "            COUNT(DISTINCT nu_cpf_cnpj) as total_cnpjs,\n",
    "            COUNT(DISTINCT cd_indicio) as total_indicios_distintos,\n",
    "            SUM(CASE WHEN cd_atual = 1 THEN 1 ELSE 0 END) as indicios_atuais,\n",
    "            SUM(CASE WHEN cd_uf = 'SC' THEN 1 ELSE 0 END) as indicios_sc\n",
    "        FROM neaf.empresa_indicio\n",
    "    \"\"\").collect()[0]\n",
    "    \n",
    "    print(f\"   Total de registros: {stats_indicios['total_registros']:,}\")\n",
    "    print(f\"   Total de CNPJs √∫nicos: {stats_indicios['total_cnpjs']:,}\")\n",
    "    print(f\"   Ind√≠cios distintos: {stats_indicios['total_indicios_distintos']}\")\n",
    "    print(f\"   Ind√≠cios atuais (cd_atual=1): {stats_indicios['indicios_atuais']:,}\")\n",
    "    print(f\"   Ind√≠cios SC (cd_uf='SC'): {stats_indicios['indicios_sc']:,}\")\n",
    "    \n",
    "    # Distribui√ß√£o por tipo de ind√≠cio\n",
    "    print(\"\\nüìã Distribui√ß√£o por C√≥digo de Ind√≠cio (SC, atuais):\")\n",
    "    dist_indicios = spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            cd_indicio,\n",
    "            COUNT(*) as qtd,\n",
    "            COUNT(DISTINCT nu_cpf_cnpj) as qtd_cnpjs,\n",
    "            ROUND(AVG(nu_score), 2) as score_medio\n",
    "        FROM neaf.empresa_indicio\n",
    "        WHERE cd_uf = 'SC' AND cd_atual = 1\n",
    "        GROUP BY cd_indicio\n",
    "        ORDER BY qtd DESC\n",
    "        LIMIT 20\n",
    "    \"\"\").toPandas()\n",
    "    \n",
    "    for _, row in dist_indicios.iterrows():\n",
    "        print(f\"   Ind√≠cio {int(row['cd_indicio']):>3}: {int(row['qtd']):>8,} ocorr√™ncias | {int(row['qtd_cnpjs']):>8,} CNPJs | score m√©dio: {row['score_medio']:.2f}\")\n",
    "    \n",
    "    # Verificar os ind√≠cios 11 (DIME) e 12 (PGDAS)\n",
    "    print(\"\\nüîç An√°lise dos Ind√≠cios 11 (DIME) e 12 (PGDAS):\")\n",
    "    ind_11_12 = spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            cd_indicio,\n",
    "            COUNT(*) as qtd,\n",
    "            COUNT(DISTINCT nu_cpf_cnpj) as qtd_cnpjs,\n",
    "            ROUND(AVG(nu_score), 2) as score_medio,\n",
    "            MIN(nu_score) as score_min,\n",
    "            MAX(nu_score) as score_max\n",
    "        FROM neaf.empresa_indicio\n",
    "        WHERE cd_uf = 'SC' \n",
    "          AND cd_atual = 1\n",
    "          AND cd_indicio IN (11, 12)\n",
    "        GROUP BY cd_indicio\n",
    "    \"\"\").toPandas()\n",
    "    \n",
    "    if len(ind_11_12) > 0:\n",
    "        for _, row in ind_11_12.iterrows():\n",
    "            tipo = \"DIME (omiss√£o)\" if row['cd_indicio'] == 11 else \"PGDAS (omiss√£o)\"\n",
    "            print(f\"   Ind√≠cio {int(row['cd_indicio'])} ({tipo}):\")\n",
    "            print(f\"      Ocorr√™ncias: {int(row['qtd']):,}\")\n",
    "            print(f\"      CNPJs √∫nicos: {int(row['qtd_cnpjs']):,}\")\n",
    "            print(f\"      Score m√©dio: {row['score_medio']:.2f}\")\n",
    "            print(f\"      Score range: {row['score_min']:.2f} - {row['score_max']:.2f}\")\n",
    "    else:\n",
    "        print(\"   ‚ö†Ô∏è Nenhum registro encontrado para ind√≠cios 11 e 12\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Erro: {str(e)[:300]}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 5. VALIDA√á√ÉO: MATRIZ DE PER√çODOS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"5. VALIDA√á√ÉO: MATRIZ DE PER√çODOS (202101-202509)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    print(\"\\nüìÖ Per√≠odos esperados: 57 per√≠odos (202101 a 202509)\")\n",
    "    print(\"   2021: 12 meses | 2022: 12 meses | 2023: 12 meses | 2024: 12 meses | 2025: 9 meses\")\n",
    "    \n",
    "    # Verificar se todos os per√≠odos existem nas fontes\n",
    "    print(\"\\nüîç Verificando cobertura de per√≠odos na DIME:\")\n",
    "    periodos_dime = spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            nu_per_ref,\n",
    "            COUNT(DISTINCT nu_cnpj) as qtd_cnpjs\n",
    "        FROM usr_sat_ods.ods_decl_dime_raw\n",
    "        WHERE nu_per_ref BETWEEN 202101 AND 202509\n",
    "        GROUP BY nu_per_ref\n",
    "        ORDER BY nu_per_ref\n",
    "    \"\"\").toPandas()\n",
    "    \n",
    "    periodos_esperados = list(range(202101, 202113)) + list(range(202201, 202213)) + \\\n",
    "                        list(range(202301, 202313)) + list(range(202401, 202413)) + \\\n",
    "                        list(range(202501, 202510))\n",
    "    \n",
    "    periodos_encontrados = periodos_dime['nu_per_ref'].tolist()\n",
    "    periodos_faltantes = set(periodos_esperados) - set(periodos_encontrados)\n",
    "    \n",
    "    if periodos_faltantes:\n",
    "        print(f\"   ‚ö†Ô∏è Per√≠odos FALTANTES na DIME: {sorted(periodos_faltantes)}\")\n",
    "    else:\n",
    "        print(f\"   ‚úÖ Todos os 57 per√≠odos presentes na DIME\")\n",
    "    \n",
    "    print(f\"   üìä Total de per√≠odos encontrados: {len(periodos_encontrados)}\")\n",
    "    \n",
    "    # Mesma verifica√ß√£o para PGDAS\n",
    "    print(\"\\nüîç Verificando cobertura de per√≠odos no PGDAS:\")\n",
    "    periodos_pgdas = spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            nu_per_ref,\n",
    "            COUNT(DISTINCT nu_cnpj_matriz) as qtd_cnpjs\n",
    "        FROM usr_sat_ods.vw_sna_pgdasd_grupo_empresarial\n",
    "        WHERE nu_per_ref BETWEEN 202101 AND 202509\n",
    "        GROUP BY nu_per_ref\n",
    "        ORDER BY nu_per_ref\n",
    "    \"\"\").toPandas()\n",
    "    \n",
    "    periodos_encontrados_pgdas = periodos_pgdas['nu_per_ref'].tolist()\n",
    "    periodos_faltantes_pgdas = set(periodos_esperados) - set(periodos_encontrados_pgdas)\n",
    "    \n",
    "    if periodos_faltantes_pgdas:\n",
    "        print(f\"   ‚ö†Ô∏è Per√≠odos FALTANTES no PGDAS: {sorted(periodos_faltantes_pgdas)}\")\n",
    "    else:\n",
    "        print(f\"   ‚úÖ Todos os 57 per√≠odos presentes no PGDAS\")\n",
    "    \n",
    "    print(f\"   üìä Total de per√≠odos encontrados: {len(periodos_encontrados_pgdas)}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Erro: {str(e)[:300]}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 6. AN√ÅLISE DE OMISS√ïES: L√ìGICA DE C√ÅLCULO\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"6. AN√ÅLISE DE OMISS√ïES: VALIDA√á√ÉO DA L√ìGICA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    print(\"\\nüîç Testando a l√≥gica de c√°lculo de omiss√µes:\")\n",
    "    print(\"   (Pegando 3 CNPJs aleat√≥rios e calculando suas omiss√µes)\")\n",
    "    \n",
    "    # Pegar 3 CNPJs aleat√≥rios que t√™m declara√ß√µes\n",
    "    cnpjs_teste = spark.sql(\"\"\"\n",
    "        SELECT DISTINCT nu_cnpj\n",
    "        FROM usr_sat_ods.ods_decl_dime_raw\n",
    "        WHERE nu_per_ref BETWEEN 202101 AND 202509\n",
    "        ORDER BY RAND()\n",
    "        LIMIT 3\n",
    "    \"\"\").toPandas()['nu_cnpj'].tolist()\n",
    "    \n",
    "    for cnpj in cnpjs_teste:\n",
    "        print(f\"\\n   üìä CNPJ: {cnpj}\")\n",
    "        \n",
    "        # Calcular per√≠odos declarados e omissos\n",
    "        analise = spark.sql(f\"\"\"\n",
    "            WITH periodos_esperados AS (\n",
    "                SELECT periodo FROM (\n",
    "                    SELECT 202101 AS periodo UNION ALL SELECT 202102 UNION ALL SELECT 202103 UNION ALL\n",
    "                    SELECT 202104 UNION ALL SELECT 202105 UNION ALL SELECT 202106 UNION ALL SELECT 202107 UNION ALL\n",
    "                    SELECT 202108 UNION ALL SELECT 202109 UNION ALL SELECT 202110 UNION ALL SELECT 202111 UNION ALL\n",
    "                    SELECT 202112 UNION ALL SELECT 202201 UNION ALL SELECT 202202 UNION ALL SELECT 202203 UNION ALL\n",
    "                    SELECT 202204 UNION ALL SELECT 202205 UNION ALL SELECT 202206 UNION ALL SELECT 202207 UNION ALL\n",
    "                    SELECT 202208 UNION ALL SELECT 202209 UNION ALL SELECT 202210 UNION ALL SELECT 202211 UNION ALL\n",
    "                    SELECT 202212 UNION ALL SELECT 202301 UNION ALL SELECT 202302 UNION ALL SELECT 202303 UNION ALL\n",
    "                    SELECT 202304 UNION ALL SELECT 202305 UNION ALL SELECT 202306 UNION ALL SELECT 202307 UNION ALL\n",
    "                    SELECT 202308 UNION ALL SELECT 202309 UNION ALL SELECT 202310 UNION ALL SELECT 202311 UNION ALL\n",
    "                    SELECT 202312 UNION ALL SELECT 202401 UNION ALL SELECT 202402 UNION ALL SELECT 202403 UNION ALL\n",
    "                    SELECT 202404 UNION ALL SELECT 202405 UNION ALL SELECT 202406 UNION ALL SELECT 202407 UNION ALL\n",
    "                    SELECT 202408 UNION ALL SELECT 202409 UNION ALL SELECT 202410 UNION ALL SELECT 202411 UNION ALL\n",
    "                    SELECT 202412 UNION ALL SELECT 202501 UNION ALL SELECT 202502 UNION ALL SELECT 202503 UNION ALL\n",
    "                    SELECT 202504 UNION ALL SELECT 202505 UNION ALL SELECT 202506 UNION ALL SELECT 202507 UNION ALL\n",
    "                    SELECT 202508 UNION ALL SELECT 202509\n",
    "                ) p\n",
    "            ),\n",
    "            periodos_declarados AS (\n",
    "                SELECT DISTINCT nu_per_ref AS periodo\n",
    "                FROM usr_sat_ods.ods_decl_dime_raw\n",
    "                WHERE nu_cnpj = '{cnpj}'\n",
    "                  AND nu_per_ref BETWEEN 202101 AND 202509\n",
    "            )\n",
    "            SELECT \n",
    "                COUNT(DISTINCT pe.periodo) as total_periodos,\n",
    "                COUNT(DISTINCT pd.periodo) as periodos_declarados,\n",
    "                COUNT(DISTINCT pe.periodo) - COUNT(DISTINCT pd.periodo) as periodos_omissos\n",
    "            FROM periodos_esperados pe\n",
    "            LEFT JOIN periodos_declarados pd ON pe.periodo = pd.periodo\n",
    "        \"\"\").collect()[0]\n",
    "        \n",
    "        print(f\"      Total de per√≠odos: {analise['total_periodos']}\")\n",
    "        print(f\"      Per√≠odos declarados: {analise['periodos_declarados']}\")\n",
    "        print(f\"      Per√≠odos omissos: {analise['periodos_omissos']}\")\n",
    "        \n",
    "        # Mostrar alguns per√≠odos declarados\n",
    "        periodos_dec = spark.sql(f\"\"\"\n",
    "            SELECT nu_per_ref, VL_FATURAMENTO, VL_TOT_CRED, VL_TOT_DEB\n",
    "            FROM usr_sat_ods.ods_decl_dime_raw\n",
    "            WHERE nu_cnpj = '{cnpj}'\n",
    "              AND nu_per_ref BETWEEN 202101 AND 202509\n",
    "            ORDER BY nu_per_ref DESC\n",
    "            LIMIT 5\n",
    "        \"\"\").toPandas()\n",
    "        \n",
    "        print(f\"      √öltimas 5 declara√ß√µes:\")\n",
    "        for _, row in periodos_dec.iterrows():\n",
    "            print(f\"         {row['nu_per_ref']}: Fat={row['VL_FATURAMENTO']:.2f}, Cred={row['VL_TOT_CRED']:.2f}, Deb={row['VL_TOT_DEB']:.2f}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Erro: {str(e)[:300]}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 7. RESUMO E CONCLUS√ïES\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"7. RESUMO E CONCLUS√ïES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\"\"\n",
    "üéØ PRINCIPAIS ACHADOS:\n",
    "\n",
    "1Ô∏è‚É£ CRIT√âRIO DE DECLARA√á√ÉO ZERADA (DIME):\n",
    "   ‚Ä¢ Crit√©rio atual: VL_FATURAMENTO = 0 OU (VL_TOT_CRED = 0 AND VL_TOT_DEB = 0)\n",
    "   ‚Ä¢ Este crit√©rio est√° CORRETO e captura declara√ß√µes com \"sem movimento\"\n",
    "   \n",
    "2Ô∏è‚É£ CRIT√âRIO DE DECLARA√á√ÉO ZERADA (PGDAS):\n",
    "   ‚Ä¢ Crit√©rio atual: vl_rec_bruta = 0\n",
    "   ‚Ä¢ Crit√©rio simples e direto, est√° CORRETO\n",
    "   \n",
    "3Ô∏è‚É£ MATRIZ DE PER√çODOS:\n",
    "   ‚Ä¢ Verificar se todos os 57 per√≠odos (202101-202509) est√£o presentes\n",
    "   ‚Ä¢ Validar se h√° \"buracos\" na s√©rie temporal\n",
    "   \n",
    "4Ô∏è‚É£ IND√çCIOS DE FRAUDE:\n",
    "   ‚Ä¢ Ind√≠cio 11 = omiss√£o DIME (Regime Normal)\n",
    "   ‚Ä¢ Ind√≠cio 12 = omiss√£o PGDAS (Simples Nacional)\n",
    "   ‚Ä¢ Verificar consist√™ncia entre as flags e os ind√≠cios\n",
    "   \n",
    "5Ô∏è‚É£ REGIME DE APURA√á√ÉO:\n",
    "   ‚Ä¢ Filtro por regime ATUAL est√° correto\n",
    "   ‚Ä¢ Empresas podem ter mudado de regime ao longo do tempo\n",
    "   \n",
    "‚ö†Ô∏è POSS√çVEIS PROBLEMAS A INVESTIGAR:\n",
    "\n",
    "1. Se h√° per√≠odos faltantes nas fontes originais\n",
    "2. Se a l√≥gica de \"omiss√£o\" est√° considerando per√≠odos corretos (57 vs 8?)\n",
    "3. Se h√° CNPJs duplicados ou com formatos diferentes\n",
    "4. Se os filtros de cd_uf='SC' e cd_atual=1 est√£o corretos para ind√≠cios\n",
    "\n",
    "üí° PR√ìXIMOS PASSOS:\n",
    "   ‚Ä¢ Com base nos resultados acima, ajustar os SQLs se necess√°rio\n",
    "   ‚Ä¢ Validar especialmente a contagem de per√≠odos (57 per√≠odos esperados)\n",
    "   ‚Ä¢ Verificar se \"total_periodos\" nas tabelas auxiliares est√° correto\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FIM DO DIAGN√ìSTICO DAS FONTES ORIGINAIS\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a84afd59-5f20-405f-8036-f8210194c525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "INVESTIGA√á√ÉO: PROBLEMAS CR√çTICOS IDENTIFICADOS\n",
      "================================================================================\n",
      "\n",
      "============================================================\n",
      "1. PROBLEMA CR√çTICO: NULL vs ZERO (DIME)\n",
      "============================================================\n",
      "\n",
      "üîç Analisando tratamento de NULLs:\n",
      "\n",
      "   Total de registros: 4,428,586\n",
      "\n",
      "   üìä NULLs:\n",
      "      Faturamento NULL: 1,633,194 (36.9%)\n",
      "      Cr√©dito NULL: 1,994,781 (45.0%)\n",
      "      D√©bito NULL: 2,185,371 (49.3%)\n",
      "\n",
      "   üìä Zeros:\n",
      "      Faturamento = 0: 457,812 (10.3%)\n",
      "      Cr√©dito = 0: 0 (0.0%)\n",
      "      D√©bito = 0: 0 (0.0%)\n",
      "\n",
      "   ‚ö†Ô∏è COMPARA√á√ÉO DE CRIT√âRIOS:\n",
      "      Crit√©rio ATUAL (sem COALESCE): 457,812 (10.3%)\n",
      "      Crit√©rio CORRETO (com COALESCE): 2,207,414 (49.8%)\n",
      "      üî¥ DIFEREN√áA: 0 registros afetados por NULLs!\n",
      "\n",
      "============================================================\n",
      "2. INCONSIST√äNCIA: IND√çCIOS DA FONTE vs FLAGS NAS TABELAS\n",
      "============================================================\n",
      "\n",
      "üîç Comparando ind√≠cios originais com flags nas tabelas auxiliares:\n",
      "\n",
      "üìä FONTE ORIGINAL (neaf.empresa_indicio):\n",
      "   Ind√≠cio 12 (PGDAS (12)): 6,005 CNPJs\n",
      "   Ind√≠cio 11 (DIME (11)): 13,160 CNPJs\n",
      "\n",
      "üìä TABELA cancel_zero_normal:\n",
      "   flag_omissao_dime_indicio = 1: 12,497\n",
      "   flag_omissao_pgdas_indicio = 1: 675 ‚ö†Ô∏è (deveria ser 0)\n",
      "\n",
      "üìä TABELA cancel_zero_simples:\n",
      "   flag_omissao_dime_indicio = 1: 1 ‚ö†Ô∏è (deveria ser 0)\n",
      "   flag_omissao_pgdas_indicio = 1: 4,723\n",
      "\n",
      "üìä TABELA credito_dime_completo:\n",
      "   flag_omissao_dime_normal = 1: 3,430\n",
      "   flag_omissao_pgdas_simples = 1: 82\n",
      "\n",
      "üî¥ AN√ÅLISE DA DISCREP√ÇNCIA:\n",
      "   DIME (ind√≠cio 11):\n",
      "      Fonte original: 13,160\n",
      "      cancel_zero_normal: 12,497\n",
      "      credito_dime_completo: 3,430\n",
      "      ‚ùå Perda: 9,730 CNPJs (73.9%)\n",
      "\n",
      "   PGDAS (ind√≠cio 12):\n",
      "      Fonte original: 6,005\n",
      "      cancel_zero_simples: 4,723\n",
      "      credito_dime_completo: 82\n",
      "      ‚ùå Perda: 5,923 CNPJs (98.6%)\n",
      "\n",
      "============================================================\n",
      "3. INVESTIGAR: PERDA DE DADOS NO JOIN\n",
      "============================================================\n",
      "\n",
      "üîç Analisando CNPJs com ind√≠cio 11 (DIME) que n√£o aparecem na tabela final:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   üìä CNPJs com ind√≠cio 11 na fonte: 13,160\n",
      "   üìä CNPJs com flag na tabela final: 3,430\n",
      "   ‚ùå CNPJs perdidos: 9,730 (73.9%)\n",
      "\n",
      "üîç Verificando se CNPJs com ind√≠cio 11 t√™m cr√©dito acumulado:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Total com ind√≠cio 11: 13,160\n",
      "   Total na credito_dime_completo: 54,777\n",
      "   Matches (JOIN): 3,413\n",
      "   ‚ùå CNPJs com ind√≠cio 11 MAS SEM cr√©dito: 9,747\n",
      "\n",
      "üí° CONCLUS√ÉO:\n",
      "   üî¥ PROBLEMA IDENTIFICADO: 9,747 empresas t√™m ind√≠cio de omiss√£o\n",
      "      mas N√ÉO aparecem na credito_dime_completo porque n√£o t√™m cr√©dito acumulado!\n",
      "      Isso significa que a tabela credito_dime_completo S√ì inclui empresas COM cr√©dito,\n",
      "      mas deveria incluir TODAS as empresas com indica√ß√µes de fraude!\n",
      "\n",
      "============================================================\n",
      "4. PERDA NO JOIN: cancel_zero_normal ‚Üí credito_dime_completo\n",
      "============================================================\n",
      "\n",
      "üîç Analisando perda de empresas no pipeline:\n",
      "   ‚ùå Erro: [TABLE_OR_VIEW_NOT_FOUND] The table or view `dual` cannot be found. Verify the spelling and correctness of the schema and catalog.\n",
      "If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\n",
      "To tolerate the error on drop\n",
      "\n",
      "============================================================\n",
      "5. RESUMO: PROBLEMAS IDENTIFICADOS E SOLU√á√ïES\n",
      "============================================================\n",
      "\n",
      "üî¥ PROBLEMA 1: TRATAMENTO DE NULL\n",
      "   ‚ùå Situa√ß√£o atual: Crit√©rio ignora valores NULL\n",
      "   ‚úÖ Solu√ß√£o: Usar COALESCE(campo, 0) em todos os crit√©rios de \"zerado\"\n",
      "   \n",
      "   SQL CORRETO:\n",
      "   CASE \n",
      "       WHEN COALESCE(VL_FATURAMENTO, 0) = 0 THEN 1\n",
      "       WHEN COALESCE(VL_TOT_CRED, 0) = 0 AND COALESCE(VL_TOT_DEB, 0) = 0 THEN 1\n",
      "       ELSE 0 \n",
      "   END AS declaracao_zerada\n",
      "\n",
      "üî¥ PROBLEMA 2: PERDA DE EMPRESAS NO JOIN\n",
      "   ‚ùå Situa√ß√£o atual: credito_dime_completo s√≥ inclui empresas COM cr√©dito\n",
      "   ‚úÖ Solu√ß√£o: Mudar a base da query para incluir TODAS as empresas do cadastro,\n",
      "              n√£o s√≥ as que t√™m cr√©dito acumulado\n",
      "   \n",
      "   SQL CORRETO:\n",
      "   - Base deve ser: usr_sat_ods.vw_cad_contrib (cadastro completo)\n",
      "   - Fazer LEFT JOIN com dados de cr√©dito\n",
      "   - Fazer LEFT JOIN com dados de enriquecimento\n",
      "\n",
      "üî¥ PROBLEMA 3: INCONSIST√äNCIA DE FLAGS\n",
      "   ‚ùå Situa√ß√£o atual: Ind√≠cios da fonte (13,160 DIME, 6,005 PGDAS) ‚â† Flags finais (3,430 DIME, 82 PGDAS)\n",
      "   ‚úÖ Solu√ß√£o: Ap√≥s corrigir Problema 2, as flags devem bater com os ind√≠cios da fonte\n",
      "\n",
      "üî¥ PROBLEMA 4: FLAGS COM NOMENCLATURA ERRADA\n",
      "   ‚ùå Situa√ß√£o atual: flag_omissao_dime_simples e flag_omissao_pgdas_normal existem (imposs√≠vel!)\n",
      "   ‚úÖ Solu√ß√£o: Remover essas colunas e manter apenas:\n",
      "              - flag_omissao_dime_normal (ou flag_omissao_dime)\n",
      "              - flag_omissao_pgdas_simples (ou flag_omissao_pgdas)\n",
      "\n",
      "üí° A√á√ÉO IMEDIATA:\n",
      "   1. Corrigir SQL das tabelas auxiliares (cancel_zero_*) com COALESCE\n",
      "   2. Corrigir SQL da credito_dime_completo mudando a base para o cadastro completo\n",
      "   3. Remover colunas de flags incorretas\n",
      "   4. Re-executar toda a pipeline\n",
      "   5. Validar se os n√∫meros batem com a fonte original\n",
      "\n",
      "\n",
      "================================================================================\n",
      "FIM DA INVESTIGA√á√ÉO\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "INVESTIGA√á√ÉO APROFUNDADA: PROBLEMAS IDENTIFICADOS\n",
    "Foco em: NULLs, Inconsist√™ncia de Flags e Joins\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"INVESTIGA√á√ÉO: PROBLEMAS CR√çTICOS IDENTIFICADOS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ============================================================================\n",
    "# 1. PROBLEMA CR√çTICO: NULL vs ZERO na DIME\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"1. PROBLEMA CR√çTICO: NULL vs ZERO (DIME)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    print(\"\\nüîç Analisando tratamento de NULLs:\")\n",
    "    \n",
    "    null_analysis = spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            COUNT(*) as total,\n",
    "            \n",
    "            -- NULLs\n",
    "            SUM(CASE WHEN VL_FATURAMENTO IS NULL THEN 1 ELSE 0 END) as fat_null,\n",
    "            SUM(CASE WHEN VL_TOT_CRED IS NULL THEN 1 ELSE 0 END) as cred_null,\n",
    "            SUM(CASE WHEN VL_TOT_DEB IS NULL THEN 1 ELSE 0 END) as deb_null,\n",
    "            \n",
    "            -- Zeros\n",
    "            SUM(CASE WHEN VL_FATURAMENTO = 0 THEN 1 ELSE 0 END) as fat_zero,\n",
    "            SUM(CASE WHEN VL_TOT_CRED = 0 THEN 1 ELSE 0 END) as cred_zero,\n",
    "            SUM(CASE WHEN VL_TOT_DEB = 0 THEN 1 ELSE 0 END) as deb_zero,\n",
    "            \n",
    "            -- Crit√©rio ATUAL (ERRADO - n√£o trata NULL)\n",
    "            SUM(CASE \n",
    "                WHEN VL_FATURAMENTO = 0 THEN 1\n",
    "                WHEN VL_TOT_CRED = 0 AND VL_TOT_DEB = 0 THEN 1\n",
    "                ELSE 0 \n",
    "            END) as criterio_atual,\n",
    "            \n",
    "            -- Crit√©rio CORRETO (trata NULL como zero)\n",
    "            SUM(CASE \n",
    "                WHEN COALESCE(VL_FATURAMENTO, 0) = 0 THEN 1\n",
    "                WHEN COALESCE(VL_TOT_CRED, 0) = 0 AND COALESCE(VL_TOT_DEB, 0) = 0 THEN 1\n",
    "                ELSE 0 \n",
    "            END) as criterio_corrigido,\n",
    "            \n",
    "            -- Casos onde NULL faz diferen√ßa\n",
    "            SUM(CASE \n",
    "                WHEN (COALESCE(VL_FATURAMENTO, 0) = 0 OR \n",
    "                      (COALESCE(VL_TOT_CRED, 0) = 0 AND COALESCE(VL_TOT_DEB, 0) = 0))\n",
    "                     AND NOT (VL_FATURAMENTO = 0 OR (VL_TOT_CRED = 0 AND VL_TOT_DEB = 0))\n",
    "                THEN 1 ELSE 0 \n",
    "            END) as diferenca_null\n",
    "            \n",
    "        FROM usr_sat_ods.ods_decl_dime_raw\n",
    "        WHERE nu_per_ref BETWEEN 202101 AND 202509\n",
    "    \"\"\").collect()[0]\n",
    "    \n",
    "    total = null_analysis['total']\n",
    "    print(f\"\\n   Total de registros: {total:,}\")\n",
    "    print(f\"\\n   üìä NULLs:\")\n",
    "    print(f\"      Faturamento NULL: {null_analysis['fat_null']:,} ({null_analysis['fat_null']/total*100:.1f}%)\")\n",
    "    print(f\"      Cr√©dito NULL: {null_analysis['cred_null']:,} ({null_analysis['cred_null']/total*100:.1f}%)\")\n",
    "    print(f\"      D√©bito NULL: {null_analysis['deb_null']:,} ({null_analysis['deb_null']/total*100:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\n   üìä Zeros:\")\n",
    "    print(f\"      Faturamento = 0: {null_analysis['fat_zero']:,} ({null_analysis['fat_zero']/total*100:.1f}%)\")\n",
    "    print(f\"      Cr√©dito = 0: {null_analysis['cred_zero']:,} ({null_analysis['cred_zero']/total*100:.1f}%)\")\n",
    "    print(f\"      D√©bito = 0: {null_analysis['deb_zero']:,} ({null_analysis['deb_zero']/total*100:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\n   ‚ö†Ô∏è COMPARA√á√ÉO DE CRIT√âRIOS:\")\n",
    "    print(f\"      Crit√©rio ATUAL (sem COALESCE): {null_analysis['criterio_atual']:,} ({null_analysis['criterio_atual']/total*100:.1f}%)\")\n",
    "    print(f\"      Crit√©rio CORRETO (com COALESCE): {null_analysis['criterio_corrigido']:,} ({null_analysis['criterio_corrigido']/total*100:.1f}%)\")\n",
    "    print(f\"      üî¥ DIFEREN√áA: {null_analysis['diferenca_null']:,} registros afetados por NULLs!\")\n",
    "    \n",
    "    if null_analysis['diferenca_null'] > 0:\n",
    "        print(f\"\\n   ‚ùå PROBLEMA CONFIRMADO: O crit√©rio atual IGNORA {null_analysis['diferenca_null']:,} declara√ß√µes zeradas por causa de NULLs!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Erro: {str(e)[:300]}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 2. INCONSIST√äNCIA: IND√çCIOS vs FLAGS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"2. INCONSIST√äNCIA: IND√çCIOS DA FONTE vs FLAGS NAS TABELAS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    print(\"\\nüîç Comparando ind√≠cios originais com flags nas tabelas auxiliares:\")\n",
    "    \n",
    "    # Ind√≠cios na fonte original\n",
    "    print(\"\\nüìä FONTE ORIGINAL (neaf.empresa_indicio):\")\n",
    "    indicios_fonte = spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            cd_indicio,\n",
    "            COUNT(DISTINCT nu_cpf_cnpj) as qtd_cnpjs\n",
    "        FROM neaf.empresa_indicio\n",
    "        WHERE cd_uf = 'SC' \n",
    "          AND cd_atual = 1\n",
    "          AND cd_indicio IN (11, 12)\n",
    "        GROUP BY cd_indicio\n",
    "    \"\"\").toPandas()\n",
    "    \n",
    "    for _, row in indicios_fonte.iterrows():\n",
    "        tipo = \"DIME (11)\" if row['cd_indicio'] == 11 else \"PGDAS (12)\"\n",
    "        print(f\"   Ind√≠cio {int(row['cd_indicio'])} ({tipo}): {int(row['qtd_cnpjs']):,} CNPJs\")\n",
    "    \n",
    "    # Flags nas tabelas auxiliares\n",
    "    print(\"\\nüìä TABELA cancel_zero_normal:\")\n",
    "    flags_normal = spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            SUM(CASE WHEN flag_omissao_dime_indicio = 1 THEN 1 ELSE 0 END) as flag_dime,\n",
    "            SUM(CASE WHEN flag_omissao_pgdas_indicio = 1 THEN 1 ELSE 0 END) as flag_pgdas\n",
    "        FROM teste.cancel_zero_normal\n",
    "    \"\"\").collect()[0]\n",
    "    print(f\"   flag_omissao_dime_indicio = 1: {flags_normal['flag_dime']:,}\")\n",
    "    print(f\"   flag_omissao_pgdas_indicio = 1: {flags_normal['flag_pgdas']:,} ‚ö†Ô∏è (deveria ser 0)\")\n",
    "    \n",
    "    print(\"\\nüìä TABELA cancel_zero_simples:\")\n",
    "    flags_simples = spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            SUM(CASE WHEN flag_omissao_dime_indicio = 1 THEN 1 ELSE 0 END) as flag_dime,\n",
    "            SUM(CASE WHEN flag_omissao_pgdas_indicio = 1 THEN 1 ELSE 0 END) as flag_pgdas\n",
    "        FROM teste.cancel_zero_simples\n",
    "    \"\"\").collect()[0]\n",
    "    print(f\"   flag_omissao_dime_indicio = 1: {flags_simples['flag_dime']:,} ‚ö†Ô∏è (deveria ser 0)\")\n",
    "    print(f\"   flag_omissao_pgdas_indicio = 1: {flags_simples['flag_pgdas']:,}\")\n",
    "    \n",
    "    print(\"\\nüìä TABELA credito_dime_completo:\")\n",
    "    flags_completo = spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            SUM(CASE WHEN flag_omissao_dime_normal = 1 THEN 1 ELSE 0 END) as flag_dime_normal,\n",
    "            SUM(CASE WHEN flag_omissao_pgdas_simples = 1 THEN 1 ELSE 0 END) as flag_pgdas_simples\n",
    "        FROM teste.credito_dime_completo\n",
    "    \"\"\").collect()[0]\n",
    "    print(f\"   flag_omissao_dime_normal = 1: {flags_completo['flag_dime_normal']:,}\")\n",
    "    print(f\"   flag_omissao_pgdas_simples = 1: {flags_completo['flag_pgdas_simples']:,}\")\n",
    "    \n",
    "    # An√°lise da discrep√¢ncia\n",
    "    print(\"\\nüî¥ AN√ÅLISE DA DISCREP√ÇNCIA:\")\n",
    "    dime_fonte = indicios_fonte[indicios_fonte['cd_indicio'] == 11]['qtd_cnpjs'].values[0] if len(indicios_fonte[indicios_fonte['cd_indicio'] == 11]) > 0 else 0\n",
    "    pgdas_fonte = indicios_fonte[indicios_fonte['cd_indicio'] == 12]['qtd_cnpjs'].values[0] if len(indicios_fonte[indicios_fonte['cd_indicio'] == 12]) > 0 else 0\n",
    "    \n",
    "    print(f\"   DIME (ind√≠cio 11):\")\n",
    "    print(f\"      Fonte original: {dime_fonte:,}\")\n",
    "    print(f\"      cancel_zero_normal: {flags_normal['flag_dime']:,}\")\n",
    "    print(f\"      credito_dime_completo: {flags_completo['flag_dime_normal']:,}\")\n",
    "    print(f\"      ‚ùå Perda: {dime_fonte - flags_completo['flag_dime_normal']:,} CNPJs ({(dime_fonte - flags_completo['flag_dime_normal'])/dime_fonte*100:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\n   PGDAS (ind√≠cio 12):\")\n",
    "    print(f\"      Fonte original: {pgdas_fonte:,}\")\n",
    "    print(f\"      cancel_zero_simples: {flags_simples['flag_pgdas']:,}\")\n",
    "    print(f\"      credito_dime_completo: {flags_completo['flag_pgdas_simples']:,}\")\n",
    "    print(f\"      ‚ùå Perda: {pgdas_fonte - flags_completo['flag_pgdas_simples']:,} CNPJs ({(pgdas_fonte - flags_completo['flag_pgdas_simples'])/pgdas_fonte*100:.1f}%)\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Erro: {str(e)[:300]}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 3. INVESTIGAR: PROBLEMA NO JOIN\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"3. INVESTIGAR: PERDA DE DADOS NO JOIN\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    print(\"\\nüîç Analisando CNPJs com ind√≠cio 11 (DIME) que n√£o aparecem na tabela final:\")\n",
    "    \n",
    "    # CNPJs com ind√≠cio 11 na fonte\n",
    "    cnpjs_indicio11 = spark.sql(\"\"\"\n",
    "        SELECT COUNT(DISTINCT nu_cpf_cnpj) as qtd\n",
    "        FROM neaf.empresa_indicio\n",
    "        WHERE cd_uf = 'SC' \n",
    "          AND cd_atual = 1\n",
    "          AND cd_indicio = 11\n",
    "    \"\"\").collect()[0]['qtd']\n",
    "    \n",
    "    # CNPJs com flag na tabela final\n",
    "    cnpjs_flag = spark.sql(\"\"\"\n",
    "        SELECT COUNT(*) as qtd\n",
    "        FROM teste.credito_dime_completo\n",
    "        WHERE flag_omissao_dime_normal = 1\n",
    "    \"\"\").collect()[0]['qtd']\n",
    "    \n",
    "    print(f\"\\n   üìä CNPJs com ind√≠cio 11 na fonte: {cnpjs_indicio11:,}\")\n",
    "    print(f\"   üìä CNPJs com flag na tabela final: {cnpjs_flag:,}\")\n",
    "    print(f\"   ‚ùå CNPJs perdidos: {cnpjs_indicio11 - cnpjs_flag:,} ({(cnpjs_indicio11 - cnpjs_flag)/cnpjs_indicio11*100:.1f}%)\")\n",
    "    \n",
    "    # Verificar se o problema √© no join entre neaf.empresa_indicio e credito_dime_completo\n",
    "    print(\"\\nüîç Verificando se CNPJs com ind√≠cio 11 t√™m cr√©dito acumulado:\")\n",
    "    \n",
    "    join_analysis = spark.sql(\"\"\"\n",
    "        WITH indicios_11 AS (\n",
    "            SELECT DISTINCT \n",
    "                REGEXP_REPLACE(TRIM(CAST(nu_cpf_cnpj AS STRING)), '[^0-9]', '') AS cnpj_limpo\n",
    "            FROM neaf.empresa_indicio\n",
    "            WHERE cd_uf = 'SC' \n",
    "              AND cd_atual = 1\n",
    "              AND cd_indicio = 11\n",
    "        ),\n",
    "        empresas_credito AS (\n",
    "            SELECT DISTINCT \n",
    "                REGEXP_REPLACE(TRIM(CAST(nu_cnpj AS STRING)), '[^0-9]', '') AS cnpj_limpo\n",
    "            FROM teste.credito_dime_completo\n",
    "        )\n",
    "        SELECT \n",
    "            (SELECT COUNT(*) FROM indicios_11) as total_indicios,\n",
    "            (SELECT COUNT(*) FROM empresas_credito) as total_credito,\n",
    "            COUNT(*) as match_count\n",
    "        FROM indicios_11 i\n",
    "        JOIN empresas_credito c ON i.cnpj_limpo = c.cnpj_limpo\n",
    "    \"\"\").collect()[0]\n",
    "    \n",
    "    print(f\"   Total com ind√≠cio 11: {join_analysis['total_indicios']:,}\")\n",
    "    print(f\"   Total na credito_dime_completo: {join_analysis['total_credito']:,}\")\n",
    "    print(f\"   Matches (JOIN): {join_analysis['match_count']:,}\")\n",
    "    print(f\"   ‚ùå CNPJs com ind√≠cio 11 MAS SEM cr√©dito: {join_analysis['total_indicios'] - join_analysis['match_count']:,}\")\n",
    "    \n",
    "    print(\"\\nüí° CONCLUS√ÉO:\")\n",
    "    if join_analysis['total_indicios'] - join_analysis['match_count'] > 0:\n",
    "        print(f\"   üî¥ PROBLEMA IDENTIFICADO: {join_analysis['total_indicios'] - join_analysis['match_count']:,} empresas t√™m ind√≠cio de omiss√£o\")\n",
    "        print(f\"      mas N√ÉO aparecem na credito_dime_completo porque n√£o t√™m cr√©dito acumulado!\")\n",
    "        print(f\"      Isso significa que a tabela credito_dime_completo S√ì inclui empresas COM cr√©dito,\")\n",
    "        print(f\"      mas deveria incluir TODAS as empresas com indica√ß√µes de fraude!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Erro: {str(e)[:300]}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 4. AN√ÅLISE: EMPRESAS NA cancel_zero_normal MAS N√ÉO NA credito_dime_completo\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"4. PERDA NO JOIN: cancel_zero_normal ‚Üí credito_dime_completo\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    print(\"\\nüîç Analisando perda de empresas no pipeline:\")\n",
    "    \n",
    "    pipeline_analysis = spark.sql(\"\"\"\n",
    "        WITH normal_cnpjs AS (\n",
    "            SELECT DISTINCT \n",
    "                REGEXP_REPLACE(TRIM(CAST(nu_cnpj AS STRING)), '[^0-9]', '') AS cnpj_limpo\n",
    "            FROM teste.cancel_zero_normal\n",
    "            WHERE periodos_omissos > 0 OR periodos_zerados > 0\n",
    "        ),\n",
    "        completo_cnpjs AS (\n",
    "            SELECT DISTINCT \n",
    "                REGEXP_REPLACE(TRIM(CAST(nu_cnpj AS STRING)), '[^0-9]', '') AS cnpj_limpo\n",
    "            FROM teste.credito_dime_completo\n",
    "        )\n",
    "        SELECT \n",
    "            (SELECT COUNT(*) FROM normal_cnpjs) as na_normal,\n",
    "            (SELECT COUNT(*) FROM completo_cnpjs) as na_completo,\n",
    "            (SELECT COUNT(*) FROM normal_cnpjs n WHERE EXISTS (SELECT 1 FROM completo_cnpjs c WHERE c.cnpj_limpo = n.cnpj_limpo)) as match\n",
    "        FROM dual\n",
    "    \"\"\").collect()[0]\n",
    "    \n",
    "    print(f\"   CNPJs em cancel_zero_normal (com omiss√µes/zerados): {pipeline_analysis['na_normal']:,}\")\n",
    "    print(f\"   CNPJs em credito_dime_completo: {pipeline_analysis['na_completo']:,}\")\n",
    "    print(f\"   Match: {pipeline_analysis['match']:,}\")\n",
    "    print(f\"   ‚ùå Perdidos: {pipeline_analysis['na_normal'] - pipeline_analysis['match']:,} ({(pipeline_analysis['na_normal'] - pipeline_analysis['match'])/pipeline_analysis['na_normal']*100:.1f}%)\")\n",
    "    \n",
    "    # Investigar POR QUE empresas s√£o perdidas\n",
    "    print(\"\\nüîç Motivo da perda:\")\n",
    "    print(\"   A tabela credito_dime_completo usa como base:\")\n",
    "    print(\"   'dados_filtrados' que requer: vl_cred_mes_anterior > 0 AND vl_cred_mes_seguinte > 0\")\n",
    "    print(\"   OU SEJA: S√ì empresas com CR√âDITO ACUMULADO s√£o inclu√≠das!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Erro: {str(e)[:300]}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 5. RESUMO DOS PROBLEMAS E SOLU√á√ïES\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"5. RESUMO: PROBLEMAS IDENTIFICADOS E SOLU√á√ïES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\"\"\n",
    "üî¥ PROBLEMA 1: TRATAMENTO DE NULL\n",
    "   ‚ùå Situa√ß√£o atual: Crit√©rio ignora valores NULL\n",
    "   ‚úÖ Solu√ß√£o: Usar COALESCE(campo, 0) em todos os crit√©rios de \"zerado\"\n",
    "   \n",
    "   SQL CORRETO:\n",
    "   CASE \n",
    "       WHEN COALESCE(VL_FATURAMENTO, 0) = 0 THEN 1\n",
    "       WHEN COALESCE(VL_TOT_CRED, 0) = 0 AND COALESCE(VL_TOT_DEB, 0) = 0 THEN 1\n",
    "       ELSE 0 \n",
    "   END AS declaracao_zerada\n",
    "\n",
    "üî¥ PROBLEMA 2: PERDA DE EMPRESAS NO JOIN\n",
    "   ‚ùå Situa√ß√£o atual: credito_dime_completo s√≥ inclui empresas COM cr√©dito\n",
    "   ‚úÖ Solu√ß√£o: Mudar a base da query para incluir TODAS as empresas do cadastro,\n",
    "              n√£o s√≥ as que t√™m cr√©dito acumulado\n",
    "   \n",
    "   SQL CORRETO:\n",
    "   - Base deve ser: usr_sat_ods.vw_cad_contrib (cadastro completo)\n",
    "   - Fazer LEFT JOIN com dados de cr√©dito\n",
    "   - Fazer LEFT JOIN com dados de enriquecimento\n",
    "\n",
    "üî¥ PROBLEMA 3: INCONSIST√äNCIA DE FLAGS\n",
    "   ‚ùå Situa√ß√£o atual: Ind√≠cios da fonte (13,160 DIME, 6,005 PGDAS) ‚â† Flags finais (3,430 DIME, 82 PGDAS)\n",
    "   ‚úÖ Solu√ß√£o: Ap√≥s corrigir Problema 2, as flags devem bater com os ind√≠cios da fonte\n",
    "\n",
    "üî¥ PROBLEMA 4: FLAGS COM NOMENCLATURA ERRADA\n",
    "   ‚ùå Situa√ß√£o atual: flag_omissao_dime_simples e flag_omissao_pgdas_normal existem (imposs√≠vel!)\n",
    "   ‚úÖ Solu√ß√£o: Remover essas colunas e manter apenas:\n",
    "              - flag_omissao_dime_normal (ou flag_omissao_dime)\n",
    "              - flag_omissao_pgdas_simples (ou flag_omissao_pgdas)\n",
    "\n",
    "üí° A√á√ÉO IMEDIATA:\n",
    "   1. Corrigir SQL das tabelas auxiliares (cancel_zero_*) com COALESCE\n",
    "   2. Corrigir SQL da credito_dime_completo mudando a base para o cadastro completo\n",
    "   3. Remover colunas de flags incorretas\n",
    "   4. Re-executar toda a pipeline\n",
    "   5. Validar se os n√∫meros batem com a fonte original\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FIM DA INVESTIGA√á√ÉO\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7c54d3-8149-488a-8796-2c70fc2a1e72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58154b9d-56dc-4a8b-8a92-7363dfaba567",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Data Pipeline)",
   "language": "python",
   "name": "conda_data_pipeline"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
